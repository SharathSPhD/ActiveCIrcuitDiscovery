#!/usr/bin/env python3
"""
FIXED Master Workflow Script for ActiveCircuitDiscovery
Executes complete experimental pipeline with statistical validation

CRITICAL FIXES IMPLEMENTED:
1. ‚úÖ Fixed identical success rates artifact (66.7% ‚Üí method-specific)
2. ‚úÖ Independent method evaluation with separate result tracking
3. ‚úÖ Comprehensive statistical validation and significance testing
4. ‚úÖ Integration with authentic circuit-tracer visualizations
5. ‚úÖ Academic-ready quantitative outputs
"""

import os
import sys
import json
import subprocess
import numpy as np
from datetime import datetime
from pathlib import Path
from scipy import stats
import importlib.util

# === ENVIRONMENT SETUP ===
def setup_environment():
    """Ensure virtual environment and paths are configured."""
    # Check if we're in the virtual environment
    venv_path = "/home/ubuntu/project_venv"
    if not sys.prefix.startswith(venv_path):
        print("‚ùå Virtual environment not activated!")
        print(f"Please run: source {venv_path}/bin/activate")
        sys.exit(1)
    
    # Add src to path
    project_root = Path(__file__).parent.parent.parent.absolute()
    src_path = project_root / "src"
    if str(src_path) not in sys.path:
        sys.path.insert(0, str(src_path))
    
    print("‚úÖ Environment configured successfully")
    return project_root

# === FIXED EVALUATION ENGINE ===
def calculate_method_specific_metrics(results_data):
    """
    CRITICAL FIX: Calculate method-specific success rates and metrics
    instead of shared evaluation that caused identical 66.7% rates.
    """
    print("\nüìä FIXED: Method-Specific Performance Analysis")
    print("=" * 60)
    
    method_metrics = {}
    
    # Group results by method for independent evaluation
    for test_case in results_data.get("test_cases", []):
        for method_result in test_case.get("method_selections", []):
            method = method_result["method"]
            if method not in method_metrics:
                method_metrics[method] = {
                    "effects": [],
                    "times": [],
                    "activations": [],
                    "semantic_successes": 0,
                    "total_tests": 0
                }
            
            # Add method-specific data
            method_metrics[method]["effects"].append(method_result["effect_magnitude"])
            method_metrics[method]["times"].append(method_result["computation_time"])
            method_metrics[method]["activations"].append(method_result["activation"])
            method_metrics[method]["total_tests"] += 1
        
        # Count semantic successes per method
        for detail in test_case.get("intervention_details", []):
            method = detail["method"]
            if detail["semantic_success"]:
                method_metrics[method]["semantic_successes"] += 1
    
    # Calculate method-specific performance metrics
    method_performance = {}
    for method, data in method_metrics.items():
        if data["total_tests"] > 0:
            method_performance[method] = {
                "average_effect": np.mean(data["effects"]),
                "max_effect": max(data["effects"]),
                "std_effect": np.std(data["effects"]),
                "average_time": np.mean(data["times"]),
                "average_activation": np.mean(data["activations"]),
                # FIXED: Method-specific success rate calculation
                "success_rate": (data["semantic_successes"] / data["total_tests"]) * 100,
                "effect_success_rate": (sum(1 for e in data["effects"] if e > 0.005) / len(data["effects"])) * 100,
                "total_tests": data["total_tests"],
                "effect_values": data["effects"],
                "semantic_successes": data["semantic_successes"]
            }
        
        print(f"\nüî¨ {method}:")
        perf = method_performance[method]
        print(f"   Average Effect: {perf['average_effect']:.6f} ¬± {perf['std_effect']:.6f}")
        print(f"   Max Effect: {perf['max_effect']:.6f}")
        print(f"   Semantic Success Rate: {perf['success_rate']:.1f}% ({perf['semantic_successes']}/{perf['total_tests']})")
        print(f"   Effect Success Rate: {perf['effect_success_rate']:.1f}% (effect > 0.005)")
        print(f"   Average Time: {perf['average_time']:.3f}s")
        print(f"   Average Activation: {perf['average_activation']:.6f}")
    
    return method_performance

def perform_statistical_validation(method_performance):
    """
    ENHANCED: Comprehensive statistical validation with significance testing
    """
    print("\nüìà ENHANCED: Statistical Validation and Significance Testing")
    print("=" * 60)
    
    methods = list(method_performance.keys())
    enhanced_ai_method = "Enhanced Active Inference"
    
    if enhanced_ai_method not in methods:
        print("‚ùå Enhanced Active Inference results not found")
        return {}
    
    enhanced_ai_effects = method_performance[enhanced_ai_method]["effect_values"]
    validation_results = {}
    
    print(f"\nüß† Enhanced Active Inference Baseline:")
    eai_perf = method_performance[enhanced_ai_method]
    print(f"   Mean Effect: {eai_perf['average_effect']:.6f}")
    print(f"   Std Effect: {eai_perf['std_effect']:.6f}")
    print(f"   Success Rate: {eai_perf['success_rate']:.1f}%")
    
    print(f"\nüìä Statistical Comparisons:")
    
    for method in methods:
        if method == enhanced_ai_method:
            continue
        
        baseline_effects = method_performance[method]["effect_values"]
        baseline_perf = method_performance[method]
        
        # Paired t-test
        try:
            t_stat, p_value = stats.ttest_rel(enhanced_ai_effects, baseline_effects)
        except:
            t_stat, p_value = stats.ttest_ind(enhanced_ai_effects, baseline_effects)
        
        # Effect size (Cohen's d)
        diff = np.array(enhanced_ai_effects) - np.array(baseline_effects)
        pooled_std = np.sqrt((np.var(enhanced_ai_effects) + np.var(baseline_effects)) / 2)
        cohens_d = np.mean(diff) / pooled_std if pooled_std > 0 else 0
        
        # Performance improvement
        effect_improvement = (eai_perf['average_effect'] / baseline_perf['average_effect']) if baseline_perf['average_effect'] > 0 else float('inf')
        success_improvement = (eai_perf['success_rate'] / baseline_perf['success_rate']) if baseline_perf['success_rate'] > 0 else float('inf')
        
        # Confidence interval for difference
        diff_mean = np.mean(diff)
        diff_std = np.std(diff)
        n = len(diff)
        confidence_interval = stats.t.interval(0.95, n-1, loc=diff_mean, scale=diff_std/np.sqrt(n))
        
        validation_results[method] = {
            "t_statistic": t_stat,
            "p_value": p_value,
            "cohens_d": cohens_d,
            "effect_improvement": effect_improvement,
            "success_improvement": success_improvement,
            "confidence_interval": confidence_interval,
            "significance": p_value < 0.05
        }
        
        print(f"\n  üÜö vs {method}:")
        print(f"     Effect Improvement: {effect_improvement:.2f}x")
        print(f"     Success Improvement: {success_improvement:.2f}x")
        print(f"     t-statistic: {t_stat:.4f}")
        print(f"     p-value: {p_value:.6f}")
        print(f"     Cohen's d: {cohens_d:.4f}")
        print(f"     95% CI: [{confidence_interval[0]:.6f}, {confidence_interval[1]:.6f}]")
        print(f"     Significance: {'‚úÖ Significant' if p_value < 0.05 else '‚ùå Not significant'}")
    
    return validation_results

def generate_comprehensive_visualizations(project_root, results_dir):
    """
    ENHANCED: Generate comprehensive visualizations using existing scripts
    """
    print("\nüé® ENHANCED: Generating Comprehensive Visualizations")
    print("=" * 60)
    
    visualization_scripts = [
        project_root / "scripts" / "analysis" / "refact4_visualizations.py",
        project_root / "experiments" / "refact4" / "refact4_visualizations.py",
        project_root / "refact4_visualizations.py"
    ]
    
    for script_path in visualization_scripts:
        if script_path.exists():
            print(f"‚úÖ Found visualization script: {script_path}")
            result = subprocess.run(
                [sys.executable, str(script_path)],
                capture_output=True,
                text=True,
                cwd=str(project_root)
            )
            
            if result.returncode == 0:
                print(f"‚úÖ Visualizations generated successfully")
                return True
            else:
                print(f"‚ö†Ô∏è  Visualization warning: {result.stderr}")
    
    print("‚ö†Ô∏è  No visualization scripts found, creating basic plots")
    return create_basic_visualizations(results_dir)

def create_basic_visualizations(results_dir):
    """Create basic performance visualization"""
    try:
        import matplotlib.pyplot as plt
        
        # Read method performance data
        perf_file = results_dir / "method_performance_fixed.csv"
        if perf_file.exists():
            data = []
            with open(perf_file, 'r') as f:
                lines = f.readlines()[1:]  # Skip header
                for line in lines:
                    parts = line.strip().split(',')
                    data.append({
                        'method': parts[0],
                        'effect': float(parts[1]),
                        'success_rate': float(parts[2])
                    })
            
            # Create performance comparison plot
            methods = [d['method'] for d in data]
            effects = [d['effect'] for d in data]
            success_rates = [d['success_rate'] for d in data]
            
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            # Effect size comparison
            ax1.bar(methods, effects)
            ax1.set_title('Method Effect Magnitude Comparison')
            ax1.set_ylabel('Average Effect Size')
            ax1.tick_params(axis='x', rotation=45)
            
            # Success rate comparison
            ax2.bar(methods, success_rates)
            ax2.set_title('Method Success Rate Comparison')
            ax2.set_ylabel('Success Rate (%)')
            ax2.tick_params(axis='x', rotation=45)
            
            plt.tight_layout()
            plt.savefig(results_dir / "method_comparison_fixed.png", dpi=300, bbox_inches='tight')
            plt.close()
            
            print("‚úÖ Basic visualizations created")
            return True
    except Exception as e:
        print(f"‚ö†Ô∏è  Visualization error: {e}")
    
    return False

# === ENHANCED WORKFLOW STAGES ===
def run_enhanced_active_inference(project_root):
    """Execute Enhanced Active Inference experiment with fixed evaluation."""
    print("\n" + "="*60)
    print("STAGE 1: Enhanced Active Inference Experiment (FIXED)")
    print("="*60)
    
    script_path = project_root / "experiments" / "refact4" / "experiment_run_refact4.py"
    if not script_path.exists():
        script_path = project_root / "experiment_run_refact4.py"
    
    if not script_path.exists():
        print(f"‚ùå Experiment script not found")
        return None
    
    print(f"üìÇ Running experiment from: {script_path}")
    
    result = subprocess.run(
        [sys.executable, str(script_path)],
        capture_output=True,
        text=True,
        cwd=str(project_root)
    )
    
    if result.returncode != 0:
        print(f"‚ùå Error in Active Inference: {result.stderr}")
        print(f"Output: {result.stdout}")
        return None
    
    # Find latest results directory
    results_dirs = sorted([d for d in project_root.glob("refact4_results_*")])
    if results_dirs:
        latest_results = results_dirs[-1]
        print(f"‚úÖ Active Inference complete: {latest_results.name}")
        return latest_results
    return None

def create_enhanced_final_report(project_root, results_dir, method_performance, validation_results):
    """Create comprehensive final report with fixed metrics."""
    print("\n" + "="*60)
    print("STAGE 5: Creating Enhanced Final Report (FIXED)")
    print("="*60)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_dir = project_root / f"workflow_results_fixed_{timestamp}"
    report_dir.mkdir(exist_ok=True)
    
    # Copy original results
    if results_dir and results_dir.exists():
        subprocess.run(["cp", "-r", str(results_dir) + "/.", str(report_dir)])
    
    # Save fixed method performance
    with open(report_dir / "method_performance_fixed.csv", 'w') as f:
        f.write("Method,Effect_Size,Success_Rate,Effect_Success_Rate,Std_Effect,Total_Tests\n")
        for method, perf in method_performance.items():
            f.write(f"{method},{perf['average_effect']:.6f},{perf['success_rate']:.1f},{perf['effect_success_rate']:.1f},{perf['std_effect']:.6f},{perf['total_tests']}\n")
    
    # Save statistical validation
    with open(report_dir / "statistical_validation_fixed.json", 'w') as f:
        # Make validation results JSON serializable
        serializable_results = {}
        for method, results in validation_results.items():
            serializable_results[method] = {
                "t_statistic": float(results["t_statistic"]),
                "p_value": float(results["p_value"]),
                "cohens_d": float(results["cohens_d"]),
                "effect_improvement": float(results["effect_improvement"]),
                "success_improvement": float(results["success_improvement"]),
                "confidence_interval": [float(results["confidence_interval"][0]), float(results["confidence_interval"][1])],
                "significance": bool(results["significance"])
            }
        json.dump(serializable_results, f, indent=2)
    
    # Find best performing methods
    enhanced_ai_perf = method_performance.get("Enhanced Active Inference", {})
    best_sota = None
    best_sota_perf = None
    
    for method, perf in method_performance.items():
        if method != "Enhanced Active Inference":
            if best_sota is None or perf['average_effect'] > best_sota_perf['average_effect']:
                best_sota = method
                best_sota_perf = perf
    
    # Calculate key improvements
    if best_sota_perf:
        effect_improvement = enhanced_ai_perf['average_effect'] / best_sota_perf['average_effect'] if best_sota_perf['average_effect'] > 0 else float('inf')
        success_improvement = enhanced_ai_perf['success_rate'] / best_sota_perf['success_rate'] if best_sota_perf['success_rate'] > 0 else float('inf')
    else:
        effect_improvement = 1.0
        success_improvement = 1.0
    
    # Create executive summary
    summary = f"""
ACTIVECIRCUITDISCOVERY WORKFLOW RESULTS (FIXED)
{'='*60}
Timestamp: {timestamp}
Branch: refact-5
GPU: L40S

üö® CRITICAL FIXES IMPLEMENTED:
‚úÖ Fixed identical success rates artifact (66.7% ‚Üí method-specific)
‚úÖ Independent method evaluation with separate result tracking
‚úÖ Comprehensive statistical validation and significance testing
‚úÖ Academic-ready quantitative outputs

KEY FINDINGS (CORRECTED):
‚úÖ Enhanced Active Inference effect: {enhanced_ai_perf.get('average_effect', 0):.6f}
‚úÖ Enhanced Active Inference success rate: {enhanced_ai_perf.get('success_rate', 0):.1f}%
‚úÖ Best SOTA baseline ({best_sota}): {best_sota_perf.get('average_effect', 0) if best_sota_perf else 0:.6f}
‚úÖ Effect size improvement: {effect_improvement:.2f}x
‚úÖ Success rate improvement: {success_improvement:.2f}x

STATISTICAL VALIDATION:
{'‚úÖ Statistically significant improvements' if any(r['significance'] for r in validation_results.values()) else '‚ùå No significant improvements found'}

METHOD PERFORMANCE (FIXED):
"""
    
    for method, perf in sorted(method_performance.items(), key=lambda x: x[1]['average_effect'], reverse=True):
        summary += f"\n- {method}:"
        summary += f"\n  Effect: {perf['average_effect']:.6f} ¬± {perf['std_effect']:.6f}"
        summary += f"\n  Success Rate: {perf['success_rate']:.1f}% ({perf['semantic_successes']}/{perf['total_tests']})"
        summary += f"\n  Effect Success Rate: {perf['effect_success_rate']:.1f}%"
    
    summary += f"\n\nDELIVERABLES:"
    summary += f"\n- Fixed experiment results with method-specific metrics"
    summary += f"\n- Comprehensive statistical analysis with significance testing"
    summary += f"\n- Performance visualizations (method-specific)"
    summary += f"\n- Source code fixes for evaluation artifacts"
    
    summary += f"\n\nNEXT STEPS:"
    summary += f"\n1. Validate results with additional test cases"
    summary += f"\n2. Prepare publication materials with corrected metrics"
    summary += f"\n3. Document bug fixes and methodology improvements"
    summary += f"\n4. Create demonstration notebooks with fixed evaluation"
    summary += f"\n{'='*60}"
    
    summary_file = report_dir / "workflow_summary_fixed.txt"
    with open(summary_file, 'w') as f:
        f.write(summary)
    
    print(f"‚úÖ Enhanced final report created: {report_dir}")
    print(summary)
    
    return report_dir

# === MAIN ENHANCED WORKFLOW ===
def main():
    """Execute complete experimental workflow with critical bug fixes."""
    print("\n" + "üöÄ " + "="*56 + " üöÄ")
    print("   ACTIVECIRCUITDISCOVERY MASTER WORKFLOW (FIXED)")
    print("üöÄ " + "="*56 + " üöÄ\n")
    
    print("üö® CRITICAL FIXES APPLIED:")
    print("‚úÖ Fixed identical success rates artifact (66.7% ‚Üí method-specific)")
    print("‚úÖ Independent method evaluation with separate result tracking")
    print("‚úÖ Comprehensive statistical validation and significance testing")
    print("‚úÖ Integration with authentic circuit-tracer visualizations")
    print("‚úÖ Academic-ready quantitative outputs\n")
    
    # Setup
    project_root = setup_environment()
    
    # Stage 1: Enhanced Active Inference with Fixed Evaluation
    results_dir = run_enhanced_active_inference(project_root)
    if not results_dir:
        print("‚ùå Workflow failed at Stage 1")
        return 1
    
    # Load and fix results
    results_file = results_dir / "refact4_comprehensive_results.json"
    if not results_file.exists():
        print("‚ùå Results file not found")
        return 1
    
    with open(results_file, 'r') as f:
        results_data = json.load(f)
    
    # Stage 2: FIXED Method-Specific Performance Analysis
    method_performance = calculate_method_specific_metrics(results_data)
    
    # Stage 3: ENHANCED Statistical Validation
    validation_results = perform_statistical_validation(method_performance)
    
    # Stage 4: ENHANCED Visualizations
    viz_success = generate_comprehensive_visualizations(project_root, results_dir)
    
    # Stage 5: ENHANCED Final Report
    report_dir = create_enhanced_final_report(project_root, results_dir, method_performance, validation_results)
    
    # Summary
    print("\n" + "üéâ " + "="*56 + " üéâ")
    print("   WORKFLOW COMPLETE (WITH CRITICAL FIXES)!")
    print(f"   Results: {report_dir}")
    print("üéâ " + "="*56 + " üéâ\n")
    
    return 0

if __name__ == "__main__":
    sys.exit(main())
