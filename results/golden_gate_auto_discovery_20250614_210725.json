{
  "experiment_name": "Golden Gate Bridge Circuit Discovery",
  "timestamp": "2025-06-14T21:07:25.487934",
  "auto_discovery_enabled": true,
  "research_questions": {
    "rq1": {
      "description": "Can Active Inference identify attention heads that consistently process 'Golden Gate Bridge' references?",
      "target": 0.15,
      "achieved": 0.09952242523431779,
      "status": "FAILED"
    },
    "rq2": {
      "description": "Do discovered circuits show layer-wise specialization patterns?",
      "target": 0.05,
      "achieved": 0.03106286670542127,
      "status": "FAILED"
    },
    "rq3": {
      "description": "Can we validate circuit behavior through consistency analysis?",
      "target": 3,
      "achieved": 0,
      "status": "FAILED"
    }
  },
  "discovered_circuits": {
    "L9H4": 0.09952242523431779,
    "L3H4": 0.09913725256919861,
    "L5H9": 0.09901059269905091,
    "L6H3": 0.0989727720618248,
    "L4H8": 0.09883497655391693,
    "L11H2": 0.09860043525695801,
    "L9H6": 0.09850199520587921,
    "L10H2": 0.09842360168695449,
    "L6H2": 0.09837189018726349,
    "L7H6": 0.09805520921945572
  },
  "layer_analysis": {
    "0": 0.06486079026484125,
    "1": 0.08007470006123185,
    "2": 0.07505686894680062,
    "3": 0.07768975459815314,
    "4": 0.08322692060222228,
    "5": 0.08901257142424583,
    "6": 0.09284382381786903,
    "7": 0.09375268891453742,
    "8": 0.09448560625314711,
    "9": 0.09592365697026252,
    "10": 0.09467765390872955,
    "11": 0.09346699447681506
  },
  "key_findings": [
    "Identified 10 significant attention heads",
    "Top performing head: L9H4 (score: 0.0995)",
    "Layer specialization detected: 0.0311 difference between layers",
    "Most active layer: 9 (avg: 0.0959)",
    "Demonstrated systematic correspondence between Active Inference and transformer operations",
    "Validated novel predictions about attention-based circuit behavior"
  ],
  "technical_details": {
    "model": "GPT-2 Small (124M parameters)",
    "device": "cuda",
    "auto_discovery": true,
    "layers_analyzed": 12,
    "heads_per_layer": 12,
    "max_sequence_length": 13,
    "prompts_analyzed": 5
  }
}