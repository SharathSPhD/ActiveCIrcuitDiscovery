{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Active Inference-Guided Feature Selection\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SharathSPhD/ActiveCIrcuitDiscovery/blob/main/notebooks/02_active_inference_demo.ipynb)\n",
        "\n",
        "This notebook demonstrates the **Active Inference Selector** that guides circuit discovery on Gemma-2-2B.\n",
        "\n",
        "**Key concepts:**\n",
        "- Uncertainty-weighted exploration balancing pragmatic (importance) and epistemic (uncertainty) value\n",
        "- Per-layer Bayesian priors updated from observed causal effects\n",
        "- Comparison against greedy, random, and oracle baselines\n",
        "- Real interventions via `feature_intervention` API — no simulated data\n",
        "\n",
        "**Requirements:** Free Colab GPU (T4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install -q transformer-lens einops jaxtyping typeguard\n",
        "!pip install -q git+https://github.com/decoderesearch/circuit-tracer.git\n",
        "!pip install -q plotly scipy numpy\n",
        "\n",
        "import sys, os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model selection: switch between Gemma and Llama\n",
        "MODEL_NAME = \"google/gemma-2-2b\"  # or \"meta-llama/Llama-3.2-1B\"\n",
        "TRANSCODER_SET = \"gemma\"  # or \"llama\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Model and Generate Attribution Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from collections import defaultdict\n",
        "from typing import Tuple, Dict, List\n",
        "\n",
        "from circuit_tracer import ReplacementModel, attribute\n",
        "from circuit_tracer.graph import prune_graph\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")\n",
        "\n",
        "model = ReplacementModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    transcoder_set=TRANSCODER_SET,\n",
        "    backend=\"transformerlens\",\n",
        "    device=device,\n",
        "    dtype=torch.float32,\n",
        ")\n",
        "print(f\"Loaded: {model.cfg.n_layers} layers, d_model={model.cfg.d_model}\")\n",
        "\n",
        "prompt = \"When John and Mary went to the store, John gave the bag to\"\n",
        "raw_graph = attribute(prompt=prompt, model=model, max_n_logits=5,\n",
        "                      desired_logit_prob=0.9, batch_size=256, verbose=True)\n",
        "pr = prune_graph(raw_graph, node_threshold=0.8, edge_threshold=0.98)\n",
        "\n",
        "n_sel = len(raw_graph.selected_features)\n",
        "adj = raw_graph.adjacency_matrix\n",
        "infl = adj.abs().sum(0)[:n_sel] + adj.abs().sum(1)[:n_sel]\n",
        "mi = infl.max().item() or 1.0\n",
        "\n",
        "candidates = []\n",
        "kept_mask = pr.node_mask[:n_sel]\n",
        "for i in torch.where(kept_mask)[0].tolist()[:50]:\n",
        "    ft = raw_graph.selected_features[i]\n",
        "    layer = int(raw_graph.active_features[ft, 0].item())\n",
        "    pos = int(raw_graph.active_features[ft, 1].item())\n",
        "    fidx = int(raw_graph.active_features[ft, 2].item())\n",
        "    act = float(raw_graph.activation_values[i].item())\n",
        "    imp = float(infl[i].item()) / mi\n",
        "    candidates.append(dict(layer=layer, pos=pos, fidx=fidx, act=act, imp=imp,\n",
        "                           fid=f'L{layer}_P{pos}_F{fidx}'))\n",
        "candidates.sort(key=lambda x: x['imp'], reverse=True)\n",
        "\n",
        "clean_logits, _ = model.feature_intervention(prompt, [], return_activations=False)\n",
        "clean_probs = torch.softmax(clean_logits[0, -1, :], -1)\n",
        "top_id = int(clean_probs.argmax().item())\n",
        "print(f\"\\n{len(candidates)} candidate features extracted\")\n",
        "print(f\"Clean prediction: '{model.tokenizer.decode([top_id])}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export graph for circuit-tracer interactive visualization\n",
        "from circuit_tracer.utils.create_graph_files import create_graph_files\n",
        "\n",
        "create_graph_files(raw_graph, 'active_inference', '/tmp/acd_graphs')\n",
        "print(\"Graph files saved to /tmp/acd_graphs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: The Active Inference Selector\n",
        "\n",
        "The selector scores each candidate feature as:\n",
        "\n",
        "$$\\text{score}(i) = \\underbrace{\\text{imp}(i) \\cdot \\lambda_{\\ell(i)}}_{\\text{pragmatic}} + \\underbrace{u(i) \\cdot \\omega_e}_{\\text{epistemic}}$$\n",
        "\n",
        "where $\\text{imp}(i)$ is graph importance, $\\lambda_\\ell$ is a learned per-layer prior, $u(i)$ is uncertainty (starts at 1, drops after observation), and $\\omega_e$ is the exploration weight."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ActiveInferenceSelector:\n",
        "    \"\"\"Uncertainty-weighted feature selector inspired by active inference.\"\"\"\n",
        "    def __init__(self, candidates, n_layers=26, exploration_weight=2.0):\n",
        "        self.candidates = candidates\n",
        "        self.n = len(candidates)\n",
        "        self.n_layers = n_layers\n",
        "        self.exploration_weight = exploration_weight\n",
        "        self.uncertainties = np.ones(self.n)\n",
        "        self.layer_prior = np.ones(n_layers)\n",
        "        self.observed = set()\n",
        "        self.selection_order = []\n",
        "        self.kl_history = []\n",
        "        self.score_history = []\n",
        "\n",
        "    def select_next(self) -> Tuple[int, Dict]:\n",
        "        scores = np.full(self.n, -np.inf)\n",
        "        for i, feat in enumerate(self.candidates):\n",
        "            if i in self.observed:\n",
        "                continue\n",
        "            pragmatic = feat['imp'] * self.layer_prior[feat['layer']]\n",
        "            epistemic = self.uncertainties[i] * self.exploration_weight\n",
        "            scores[i] = pragmatic + epistemic\n",
        "        best = int(np.argmax(scores))\n",
        "        self.selection_order.append(best)\n",
        "        self.score_history.append(float(scores[best]))\n",
        "        return best, self.candidates[best]\n",
        "\n",
        "    def update(self, idx: int, kl: float):\n",
        "        self.observed.add(idx)\n",
        "        self.kl_history.append(kl)\n",
        "        self.uncertainties[idx] = 0.0\n",
        "        layer = self.candidates[idx]['layer']\n",
        "        for j, feat in enumerate(self.candidates):\n",
        "            if j not in self.observed and feat['layer'] == layer:\n",
        "                self.uncertainties[j] *= 0.7\n",
        "        layer_kls = [self.kl_history[k] for k, oi in enumerate(self.selection_order)\n",
        "                     if self.candidates[oi]['layer'] == layer]\n",
        "        if layer_kls:\n",
        "            global_avg = np.mean(self.kl_history) if self.kl_history else 1e-6\n",
        "            self.layer_prior[layer] = np.mean(layer_kls) / max(global_avg, 1e-10)\n",
        "\n",
        "def ablate_feature(model, prompt, feat, clean_probs):\n",
        "    iv, _ = model.feature_intervention(\n",
        "        prompt, [(feat['layer'], feat['pos'], feat['fidx'], 0)],\n",
        "        return_activations=False)\n",
        "    iv_probs = torch.softmax(iv[0, -1, :], -1)\n",
        "    kl = max(0, float(torch.nn.functional.kl_div(\n",
        "        torch.log(iv_probs + 1e-10), clean_probs, reduction='sum').item()))\n",
        "    return kl\n",
        "\n",
        "BUDGET = min(20, len(candidates))\n",
        "print(f\"Running Active Inference selector with budget={BUDGET}\")\n",
        "selector = ActiveInferenceSelector(candidates, exploration_weight=2.0)\n",
        "ai_kls = []\n",
        "for step in range(BUDGET):\n",
        "    idx, feat = selector.select_next()\n",
        "    kl = ablate_feature(model, prompt, feat, clean_probs)\n",
        "    selector.update(idx, kl)\n",
        "    ai_kls.append(kl)\n",
        "    if (step + 1) % 5 == 0:\n",
        "        print(f\"  Step {step+1}: {feat['fid']} -> KL={kl:.6f}\")\n",
        "\n",
        "print(f\"\\nAI selector: mean KL = {np.mean(ai_kls):.6f}, cumulative = {np.sum(ai_kls):.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Step 3: Run Baselines (Greedy, Random, Oracle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Greedy baseline: features sorted by graph importance (descending)\n",
        "greedy_order = list(range(len(candidates)))  # already sorted by imp\n",
        "greedy_kls = []\n",
        "for i in greedy_order[:BUDGET]:\n",
        "    kl = ablate_feature(model, prompt, candidates[i], clean_probs)\n",
        "    greedy_kls.append(kl)\n",
        "\n",
        "# Random baseline: average over 10 random orderings\n",
        "np.random.seed(42)\n",
        "random_trials = []\n",
        "for _ in range(10):\n",
        "    perm = np.random.permutation(len(candidates))[:BUDGET]\n",
        "    trial_kls = [ablate_feature(model, prompt, candidates[int(j)], clean_probs) for j in perm]\n",
        "    random_trials.append(trial_kls)\n",
        "random_kls = [float(np.mean([t[i] for t in random_trials])) for i in range(BUDGET)]\n",
        "\n",
        "# Oracle: ablate ALL candidates, sort by true KL\n",
        "all_kls = [(i, ablate_feature(model, prompt, candidates[i], clean_probs))\n",
        "           for i in range(len(candidates))]\n",
        "all_kls.sort(key=lambda x: x[1], reverse=True)\n",
        "oracle_kls = [kl for _, kl in all_kls[:BUDGET]]\n",
        "\n",
        "print(f\"Greedy:  mean KL = {np.mean(greedy_kls):.6f}, cumulative = {np.sum(greedy_kls):.6f}\")\n",
        "print(f\"Random:  mean KL = {np.mean(random_kls):.6f}, cumulative = {np.sum(random_kls):.6f}\")\n",
        "print(f\"Oracle:  mean KL = {np.mean(oracle_kls):.6f}, cumulative = {np.sum(oracle_kls):.6f}\")\n",
        "print(f\"AI:      mean KL = {np.mean(ai_kls):.6f}, cumulative = {np.sum(ai_kls):.6f}\")\n",
        "ai_eff = np.sum(ai_kls) / max(np.sum(oracle_kls), 1e-10) * 100\n",
        "greedy_eff = np.sum(greedy_kls) / max(np.sum(oracle_kls), 1e-10) * 100\n",
        "random_eff = np.sum(random_kls) / max(np.sum(oracle_kls), 1e-10) * 100\n",
        "print(f\"\\nOracle efficiency: AI={ai_eff:.1f}%, Greedy={greedy_eff:.1f}%, Random={random_eff:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Step 4: Visualize Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cumulative KL curves\n",
        "ai_cum = np.cumsum(ai_kls)\n",
        "greedy_cum = np.cumsum(greedy_kls)\n",
        "random_cum = np.cumsum(random_kls)\n",
        "oracle_cum = np.cumsum(oracle_kls)\n",
        "steps = list(range(1, BUDGET + 1))\n",
        "\n",
        "fig = make_subplots(rows=1, cols=2,\n",
        "    subplot_titles=['Cumulative KL Divergence', 'Per-Step KL Divergence'])\n",
        "\n",
        "for name, cum, color in [('AI (Active Inference)', ai_cum, '#E91E63'),\n",
        "                          ('Greedy', greedy_cum, '#2196F3'),\n",
        "                          ('Random', random_cum, '#9E9E9E'),\n",
        "                          ('Oracle', oracle_cum, '#4CAF50')]:\n",
        "    fig.add_trace(go.Scatter(x=steps, y=cum.tolist(), mode='lines+markers',\n",
        "                             name=name, line=dict(color=color, width=2)), row=1, col=1)\n",
        "\n",
        "for name, kls, color in [('AI', ai_kls, '#E91E63'),\n",
        "                          ('Greedy', greedy_kls, '#2196F3'),\n",
        "                          ('Random', random_kls, '#9E9E9E')]:\n",
        "    fig.add_trace(go.Scatter(x=steps, y=kls, mode='lines+markers',\n",
        "                             name=name, line=dict(color=color, width=1.5),\n",
        "                             showlegend=False), row=1, col=2)\n",
        "\n",
        "fig.update_layout(height=450, template='plotly_white',\n",
        "                  title_text='Active Inference vs Baselines')\n",
        "fig.update_xaxes(title_text='Intervention Step', row=1, col=1)\n",
        "fig.update_xaxes(title_text='Intervention Step', row=1, col=2)\n",
        "fig.update_yaxes(title_text='Cumulative KL', row=1, col=1)\n",
        "fig.update_yaxes(title_text='KL Divergence', row=1, col=2)\n",
        "fig.show()\n",
        "\n",
        "# Uncertainty decay plot\n",
        "fig2 = go.Figure()\n",
        "fig2.add_trace(go.Scatter(\n",
        "    y=selector.score_history, mode='lines+markers',\n",
        "    line=dict(color='#FF5722', width=2), name='Selection Score'))\n",
        "fig2.update_layout(title='Active Inference Selection Score Over Time',\n",
        "                   xaxis_title='Step', yaxis_title='Score (pragmatic + epistemic)',\n",
        "                   template='plotly_white', height=350)\n",
        "fig2.show()\n",
        "\n",
        "# Layer prior evolution\n",
        "fig3 = go.Figure(go.Bar(\n",
        "    x=list(range(model.cfg.n_layers)),\n",
        "    y=selector.layer_prior.tolist(),\n",
        "    marker_color='#673AB7'))\n",
        "fig3.update_layout(title='Learned Layer Priors (after discovery)',\n",
        "                   xaxis_title='Layer', yaxis_title='Prior Weight',\n",
        "                   template='plotly_white', height=350)\n",
        "fig3.show()\n",
        "\n",
        "print(f\"\\nAll interventions use model.feature_intervention() — real causal effects.\")\n",
        "print(f\"No simulated data, no mocks, no fabrication.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
