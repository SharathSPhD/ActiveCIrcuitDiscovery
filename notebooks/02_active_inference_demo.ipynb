{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Active Inference POMDP Agent for Circuit Discovery\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SharathSPhD/ActiveCIrcuitDiscovery/blob/main/notebooks/02_active_inference_demo.ipynb)\n",
        "\n",
        "This notebook demonstrates the **pymdp-based Active Inference POMDP agent** that guides circuit discovery on Gemma-2-2B.\n",
        "\n",
        "**Key concepts:**\n",
        "- Multi-factor POMDP with 3 hidden state factors and 3 observation modalities\n",
        "- Expected Free Energy (EFE) minimisation for intervention selection\n",
        "- Dirichlet learning of the observation model from real intervention data\n",
        "- Comparison against a bandit heuristic, greedy, random, and oracle baselines\n",
        "- All interventions use the `feature_intervention` API\n",
        "\n",
        "**Requirements:** Free Colab GPU (T4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (pin numpy<2 for transformer-lens compatibility)\n",
        "!pip install -q \"numpy>=1.26.0,<2.0\"\n",
        "!pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install -q transformer-lens einops jaxtyping typeguard\n",
        "!pip install -q git+https://github.com/safety-research/circuit-tracer.git\n",
        "!pip install -q git+https://github.com/infer-actively/pymdp.git\n",
        "!pip install -q plotly scipy\n",
        "\n",
        "import sys, os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model selection: switch between Gemma and Llama\n",
        "MODEL_NAME = \"google/gemma-2-2b\"  # or \"meta-llama/Llama-3.2-1B\"\n",
        "TRANSCODER_SET = \"gemma\"  # or \"llama\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Model and Generate Attribution Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from collections import defaultdict\n",
        "from typing import Tuple, Dict, List\n",
        "\n",
        "from circuit_tracer import ReplacementModel, attribute\n",
        "from circuit_tracer.graph import prune_graph\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")\n",
        "\n",
        "model = ReplacementModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    transcoder_set=TRANSCODER_SET,\n",
        "    backend=\"transformerlens\",\n",
        "    device=device,\n",
        "    dtype=torch.float32,\n",
        ")\n",
        "print(f\"Loaded: {model.cfg.n_layers} layers, d_model={model.cfg.d_model}\")\n",
        "\n",
        "prompt = \"When John and Mary went to the store, John gave the bag to\"\n",
        "raw_graph = attribute(prompt=prompt, model=model, max_n_logits=5,\n",
        "                      desired_logit_prob=0.9, batch_size=256, verbose=True)\n",
        "pr = prune_graph(raw_graph, node_threshold=0.8, edge_threshold=0.98)\n",
        "\n",
        "n_sel = len(raw_graph.selected_features)\n",
        "adj = raw_graph.adjacency_matrix\n",
        "infl = adj.abs().sum(0)[:n_sel] + adj.abs().sum(1)[:n_sel]\n",
        "mi = infl.max().item() or 1.0\n",
        "\n",
        "candidates = []\n",
        "kept_mask = pr.node_mask[:n_sel]\n",
        "for i in torch.where(kept_mask)[0].tolist()[:50]:\n",
        "    ft = raw_graph.selected_features[i]\n",
        "    layer = int(raw_graph.active_features[ft, 0].item())\n",
        "    pos = int(raw_graph.active_features[ft, 1].item())\n",
        "    fidx = int(raw_graph.active_features[ft, 2].item())\n",
        "    act = float(raw_graph.activation_values[i].item())\n",
        "    imp = float(infl[i].item()) / mi\n",
        "    candidates.append(dict(layer=layer, pos=pos, fidx=fidx, act=act, imp=imp,\n",
        "                           fid=f'L{layer}_P{pos}_F{fidx}'))\n",
        "candidates.sort(key=lambda x: x['imp'], reverse=True)\n",
        "\n",
        "clean_logits, _ = model.feature_intervention(prompt, [], return_activations=False)\n",
        "clean_probs = torch.softmax(clean_logits[0, -1, :], -1)\n",
        "top_id = int(clean_probs.argmax().item())\n",
        "print(f\"\\n{len(candidates)} candidate features extracted\")\n",
        "print(f\"Clean prediction: '{model.tokenizer.decode([top_id])}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export graph for circuit-tracer interactive visualization\n",
        "from circuit_tracer.utils.create_graph_files import create_graph_files\n",
        "\n",
        "create_graph_files(raw_graph, 'active_inference', '/tmp/acd_graphs')\n",
        "print(\"Graph files saved to /tmp/acd_graphs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: The Active Inference POMDP Agent\n",
        "\n",
        "The agent maintains a generative model with:\n",
        "- **3 hidden state factors**: feature importance (4 levels), layer role (3 levels), causal influence (3 levels)\n",
        "- **3 observation modalities**: KL divergence magnitude, activation magnitude, graph connectivity\n",
        "- **3 actions**: ablation, activation patching, feature steering\n",
        "\n",
        "Intervention selection minimises the **Expected Free Energy** (EFE), which decomposes into epistemic (information gain) and pragmatic (preference satisfaction) components. The observation model (A matrix) is learned online via Dirichlet updates from real intervention data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.active_inference.pomdp_agent import ActiveInferencePOMDPAgent\n",
        "\n",
        "def ablate_feature(model, prompt, feat, clean_probs):\n",
        "    iv, _ = model.feature_intervention(\n",
        "        prompt, [(feat['layer'], feat['pos'], feat['fidx'], 0)],\n",
        "        return_activations=False)\n",
        "    iv_probs = torch.softmax(iv[0, -1, :], -1)\n",
        "    kl = max(0, float(torch.nn.functional.kl_div(\n",
        "        torch.log(iv_probs + 1e-10), clean_probs, reduction='sum').item()))\n",
        "    return kl\n",
        "\n",
        "# Add graph connectivity info to candidates\n",
        "adj = raw_graph.adjacency_matrix\n",
        "for i, c in enumerate(candidates):\n",
        "    sel_idx = None\n",
        "    for j in range(len(raw_graph.selected_features)):\n",
        "        ft = raw_graph.selected_features[j]\n",
        "        if (int(raw_graph.active_features[ft, 0].item()) == c['layer'] and\n",
        "            int(raw_graph.active_features[ft, 1].item()) == c['pos'] and\n",
        "            int(raw_graph.active_features[ft, 2].item()) == c['fidx']):\n",
        "            sel_idx = j\n",
        "            break\n",
        "    if sel_idx is not None:\n",
        "        c['in_degree'] = int((adj[:, sel_idx].abs() > 0).sum().item())\n",
        "        c['out_degree'] = int((adj[sel_idx, :].abs() > 0).sum().item())\n",
        "    else:\n",
        "        c['in_degree'] = 0\n",
        "        c['out_degree'] = 0\n",
        "\n",
        "BUDGET = min(20, len(candidates))\n",
        "print(f\"Running pymdp Active Inference POMDP agent with budget={BUDGET}\")\n",
        "\n",
        "agent = ActiveInferencePOMDPAgent(n_layers=model.cfg.n_layers)\n",
        "agent.initialize()\n",
        "\n",
        "observed_fids = set()\n",
        "ai_kls = []\n",
        "for step in range(BUDGET):\n",
        "    unobserved = [c for c in candidates if c['fid'] not in observed_fids]\n",
        "    if not unobserved:\n",
        "        break\n",
        "    feat, action, efe = agent.select_intervention(unobserved)\n",
        "    kl = ablate_feature(model, prompt, feat, clean_probs)\n",
        "    agent.update_beliefs(\n",
        "        feat, kl_divergence=kl, activation_value=feat['act'],\n",
        "        graph_connectivity=feat.get('in_degree', 0) + feat.get('out_degree', 0))\n",
        "    observed_fids.add(feat['fid'])\n",
        "    ai_kls.append(kl)\n",
        "    if (step + 1) % 5 == 0:\n",
        "        print(f\"  Step {step+1}: {feat['fid']} -> KL={kl:.6f}, EFE={efe:.4f}\")\n",
        "\n",
        "print(f\"\\nPOMDP agent: mean KL = {np.mean(ai_kls):.6f}, cumulative = {np.sum(ai_kls):.6f}\")\n",
        "print(f\"Converged: {agent.is_converged}\")\n",
        "print(f\"Entropy history: {[f'{h:.3f}' for h in agent.get_belief_entropy_history()]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Step 3: Run Baselines (Greedy, Random, Oracle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Greedy baseline: features sorted by graph importance (descending)\n",
        "greedy_order = list(range(len(candidates)))  # already sorted by imp\n",
        "greedy_kls = []\n",
        "for i in greedy_order[:BUDGET]:\n",
        "    kl = ablate_feature(model, prompt, candidates[i], clean_probs)\n",
        "    greedy_kls.append(kl)\n",
        "\n",
        "# Random baseline: average over 10 random orderings\n",
        "np.random.seed(42)\n",
        "random_trials = []\n",
        "for _ in range(10):\n",
        "    perm = np.random.permutation(len(candidates))[:BUDGET]\n",
        "    trial_kls = [ablate_feature(model, prompt, candidates[int(j)], clean_probs) for j in perm]\n",
        "    random_trials.append(trial_kls)\n",
        "random_kls = [float(np.mean([t[i] for t in random_trials])) for i in range(BUDGET)]\n",
        "\n",
        "# Oracle: ablate ALL candidates, sort by true KL\n",
        "all_kls = [(i, ablate_feature(model, prompt, candidates[i], clean_probs))\n",
        "           for i in range(len(candidates))]\n",
        "all_kls.sort(key=lambda x: x[1], reverse=True)\n",
        "oracle_kls = [kl for _, kl in all_kls[:BUDGET]]\n",
        "\n",
        "print(f\"Greedy:  mean KL = {np.mean(greedy_kls):.6f}, cumulative = {np.sum(greedy_kls):.6f}\")\n",
        "print(f\"Random:  mean KL = {np.mean(random_kls):.6f}, cumulative = {np.sum(random_kls):.6f}\")\n",
        "print(f\"Oracle:  mean KL = {np.mean(oracle_kls):.6f}, cumulative = {np.sum(oracle_kls):.6f}\")\n",
        "print(f\"AI:      mean KL = {np.mean(ai_kls):.6f}, cumulative = {np.sum(ai_kls):.6f}\")\n",
        "ai_eff = np.sum(ai_kls) / max(np.sum(oracle_kls), 1e-10) * 100\n",
        "greedy_eff = np.sum(greedy_kls) / max(np.sum(oracle_kls), 1e-10) * 100\n",
        "random_eff = np.sum(random_kls) / max(np.sum(oracle_kls), 1e-10) * 100\n",
        "print(f\"\\nOracle efficiency: AI={ai_eff:.1f}%, Greedy={greedy_eff:.1f}%, Random={random_eff:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Step 4: Visualize Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cumulative KL curves\n",
        "ai_cum = np.cumsum(ai_kls)\n",
        "greedy_cum = np.cumsum(greedy_kls)\n",
        "random_cum = np.cumsum(random_kls)\n",
        "oracle_cum = np.cumsum(oracle_kls)\n",
        "steps = list(range(1, BUDGET + 1))\n",
        "\n",
        "fig = make_subplots(rows=1, cols=2,\n",
        "    subplot_titles=['Cumulative KL Divergence', 'Per-Step KL Divergence'])\n",
        "\n",
        "for name, cum, color in [('POMDP Agent', ai_cum, '#E91E63'),\n",
        "                          ('Greedy', greedy_cum, '#2196F3'),\n",
        "                          ('Random', random_cum, '#9E9E9E'),\n",
        "                          ('Oracle', oracle_cum, '#4CAF50')]:\n",
        "    fig.add_trace(go.Scatter(x=steps, y=cum.tolist(), mode='lines+markers',\n",
        "                             name=name, line=dict(color=color, width=2)), row=1, col=1)\n",
        "\n",
        "for name, kls, color in [('POMDP', ai_kls, '#E91E63'),\n",
        "                          ('Greedy', greedy_kls, '#2196F3'),\n",
        "                          ('Random', random_kls, '#9E9E9E')]:\n",
        "    fig.add_trace(go.Scatter(x=steps, y=kls, mode='lines+markers',\n",
        "                             name=name, line=dict(color=color, width=1.5),\n",
        "                             showlegend=False), row=1, col=2)\n",
        "\n",
        "fig.update_layout(height=450, template='plotly_white',\n",
        "                  title_text='POMDP Active Inference Agent vs Baselines')\n",
        "fig.update_xaxes(title_text='Intervention Step', row=1, col=1)\n",
        "fig.update_xaxes(title_text='Intervention Step', row=1, col=2)\n",
        "fig.update_yaxes(title_text='Cumulative KL', row=1, col=1)\n",
        "fig.update_yaxes(title_text='KL Divergence', row=1, col=2)\n",
        "fig.show()\n",
        "\n",
        "# Belief entropy over time\n",
        "entropy_hist = agent.get_belief_entropy_history()\n",
        "fig2 = go.Figure()\n",
        "fig2.add_trace(go.Scatter(\n",
        "    y=entropy_hist, mode='lines+markers',\n",
        "    line=dict(color='#FF5722', width=2), name='Belief Entropy'))\n",
        "fig2.update_layout(title='POMDP Agent Belief Entropy Over Time',\n",
        "                   xaxis_title='Step', yaxis_title='Total Entropy (nats)',\n",
        "                   template='plotly_white', height=350)\n",
        "fig2.show()\n",
        "\n",
        "# EFE history\n",
        "efe_hist = agent.get_efe_history()\n",
        "fig3 = go.Figure()\n",
        "fig3.add_trace(go.Scatter(\n",
        "    y=efe_hist, mode='lines+markers',\n",
        "    line=dict(color='#673AB7', width=2), name='EFE'))\n",
        "fig3.update_layout(title='Expected Free Energy Over Time',\n",
        "                   xaxis_title='Step', yaxis_title='EFE (lower = more informative)',\n",
        "                   template='plotly_white', height=350)\n",
        "fig3.show()\n",
        "\n",
        "# Feature importance ranking from posterior beliefs\n",
        "rankings = agent.get_feature_importance_ranking()\n",
        "print(\"\\nTop 10 features by inferred importance:\")\n",
        "for fid, score in rankings[:10]:\n",
        "    print(f\"  {fid}: importance = {score:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
