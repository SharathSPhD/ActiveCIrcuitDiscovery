% ---- conclusion.tex ----

This paper presented Active Circuit Discovery (\ACD), a framework that
combines attribution graph analysis with active inference for efficient
circuit discovery in large language models.
The core contribution is the formulation of circuit discovery as a POMDP,
solved by a multi-factor Active Inference agent implemented with
\texttt{pymdp}~\cite{Pymdp2022}. The agent maintains beliefs over three
hidden state factors (feature importance, layer role, and causal
influence), selects interventions by minimising Expected Free Energy,
and learns its observation model online through Dirichlet parameter
updates from real intervention data.

The framework integrates Anthropic's \texttt{circuit-tracer}
library~\cite{Anthropic2025CT} for Edge Attribution Patching with
transcoders, providing a principled decomposition of model computation
into interpretable transcoder features. All interventions use the
\texttt{feature\_intervention} API, which correctly intervenes at the
transcoder level with proper network propagation.

The evaluation spans two architectures (Gemma-2-2B~\cite{team2024gemma}
and Llama-3.2-1B~\cite{Dubey2024llama}) and four
benchmarks: IOI circuit recovery, multi-step reasoning, feature
steering, and a multi-domain benchmark spanning geography, mathematics,
science, logic, and history.  The POMDP agent is compared against a
UCB-style bandit heuristic, greedy importance ranking, random selection,
and an oracle upper bound.

Against the four hypotheses stated in Section~\ref{sec:intro}:
\textbf{H1} (efficiency) shows a positive trend on Gemma
($+$16--30\% over random) but is not statistically significant at
$\alpha=0.05$ due to small prompt counts; \textbf{H2} (causal
controllability) is accepted ($p < 10^{-15}$, binomial test on
17/100 prediction changes at $10\times$ scaling); \textbf{H3}
(discovery quality $\geq 50\%$ oracle efficiency) is accepted for
Gemma (58.3\%) but not for Llama (37.5\%); and \textbf{H4}
(cross-architecture transfer) is accepted at the qualitative level,
with all experiment types replicated on Llama-3.2-1B.

A notable finding is the task-dependent layer structure of circuits:
IOI circuits concentrate in late layers, while reasoning and logic
features concentrate in early layers, and science prompts show a
uniform distribution.  This pattern holds across both architectures.

Future work will increase prompt set sizes for statistical power,
explore multi-step planning where the agent can compose sequences of
complementary interventions, and investigate adaptive discretisation
of the observation space.  The fully open-source codebase, Colab
notebooks, and Docker container for the NVIDIA DGX Spark platform
are released to facilitate reproduction.
