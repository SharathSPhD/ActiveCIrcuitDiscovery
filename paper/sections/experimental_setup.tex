\subsection{Models and Hardware}

We evaluate \ACD{} on two transformer architectures:
\begin{itemize}
\item \textbf{Gemma-2-2B}~\cite{team2024gemma}: 26 layers, 2304-dim,
  8 heads.  Transcoders from GemmaScope
  (\texttt{mwhanna/gemma-scope-transcoders}).
\item \textbf{Llama-3.2-1B}~\cite{Dubey2024llama}: 16 layers, 2048-dim,
  32 heads with GQA.  Transcoders from
  \texttt{mntss/transcoder-Llama-3.2-1B}.
\end{itemize}
Both models use the \texttt{circuit-tracer} library~\cite{Anthropic2025CT}
with the TransformerLens backend for attribution graph construction
and transcoder-level interventions.

Experiments run on an NVIDIA DGX Spark with GB10 GPU (128\,GB unified
memory, CUDA 12.8, aarch64).  Colab notebooks reproduce results on a
free T4 GPU.

\subsection{Benchmarks}

\subsubsection{IOI Circuit Recovery}

The Indirect Object Identification task~\cite{Wang2022}
tests whether the agent can identify features causally responsible for
predicting the indirect object.  We use 5 prompts of the form
``When [A] and [B] went to the store, [A] gave the bag to \_\_''
where the correct prediction is [B].  For each prompt, we measure the
KL divergence caused by ablating each candidate feature via the
\texttt{feature\_intervention} API.

\subsubsection{Multi-step Reasoning}

Three prompts requiring compositional reasoning across two or more
hops: e.g.\ ``The capital of the country where the Eiffel Tower is
located is''.

\subsubsection{Feature Steering}

Three concept prompts (Golden Gate Bridge, Eiffel Tower, Mount Everest).
For each, we identify the top 10 features by graph importance and scale
their activations at multipliers $m \in \{0, 2, 5, 10\}$.  We measure
the KL divergence and whether the model's top prediction changes.

\subsubsection{Multi-Domain Benchmark}

Following the Initial Research Proposal, we evaluate \ACD{} across five
cognitive domains with two prompts each:
\begin{itemize}
\item \textbf{Geography}: ``The capital of France is'', ``The Golden Gate Bridge connects San Francisco to''
\item \textbf{Mathematics}: ``The square root of 64 is'', ``If 2 + 3 = 5 then 3 + 4 =''
\item \textbf{Science}: ``Water is made of hydrogen and'', ``The speed of light is approximately''
\item \textbf{Logic}: syllogistic reasoning (``All mammals are warm-blooded\ldots'')
\item \textbf{History}: ``The year World War II ended was'', ``The first person to walk on the moon was''
\end{itemize}
This allows cross-domain comparison of circuit structure and layer
distribution.

\subsection{Baselines}

\begin{itemize}
\item \textbf{Random:} Select features uniformly at random (10 trials).
\item \textbf{Greedy:} Select features in descending order of graph
  node influence (sum of absolute adjacency weights).
\item \textbf{Oracle:} Select features in descending order of
  true KL divergence (upper bound requiring all ablations).
\end{itemize}

All methods use the same \texttt{feature\_intervention} API and
KL divergence metric.  Budgets are fixed at $B=20$ interventions
per prompt.

\subsection{Metrics}

\begin{itemize}
\item \textbf{Mean KL per intervention:}  Average KL divergence across
  selected features.  Higher values indicate the method finds more
  causally important features.

\item \textbf{Cumulative KL:}  Total information gained over the
  budget.  Compared against oracle to compute efficiency.

\item \textbf{Oracle efficiency:}  $\text{Cum.~KL} / \text{Oracle~Cum.~KL}
  \times 100\%$.  Measures how close a method is to optimal.

\item \textbf{Prediction change rate:}  Fraction of steering
  experiments where the model's top-1 prediction changes.
\end{itemize}

\subsection{Active Inference Selector}

The \ACD{} selector combines graph-structural importance with
uncertainty-weighted exploration.  For each candidate feature $i$:
\[
  \text{score}(i) = \underbrace{\text{imp}(i) \cdot \lambda_{\ell(i)}}_{\text{pragmatic}}
  + \underbrace{u(i) \cdot \omega_e}_{\text{epistemic}}
\]
where $\text{imp}(i)$ is the normalized graph influence,
$\lambda_{\ell}$ is a learned per-layer prior updated after each
observation, $u(i)$ is the uncertainty (initialized to 1, reduced
after observation), and $\omega_e = 2.0$ is the exploration weight.

After ablating feature $i$ and observing KL divergence:
\begin{enumerate}
\item $u(i) \leftarrow 0$ (observed feature has no remaining uncertainty).
\item Same-layer features: $u(j) \leftarrow 0.7 \cdot u(j)$ (partial
  uncertainty reduction by transfer).
\item Layer prior $\lambda_\ell$ is updated based on the ratio of
  layer-average KL to global-average KL.
\end{enumerate}

\subsection{Reproducibility}

All code is available at
\url{https://github.com/SharathSPhD/ActiveCIrcuitDiscovery}.
Colab notebooks reproduce the main experiments on a free T4 GPU.
The experiment runner supports model selection via
\texttt{-{}-model gemma|llama|both} and benchmark selection via
\texttt{-{}-experiment ioi|steering|multistep|domain|all}.
Docker images are provided for the DGX Spark environment.  All
results derive from real model activations via the
\texttt{feature\_intervention} API---no synthetic or fabricated data.
Raw experiment outputs are stored in \texttt{results/} as JSON.
