\subsection{Models and Hardware}

We evaluate \ACD{} on Gemma-2-2B~\cite{team2024gemma}: 26 layers,
2304-dim, 8 heads.  Transcoders from GemmaScope
(\texttt{mwhanna/gemma-scope-transcoders}) provide sparse feature
decomposition via the \texttt{circuit-tracer} library with the
TransformerLens backend.

Experiments run on an NVIDIA DGX Spark with GB10 GPU (128\,GB unified
memory, CUDA 12.8, aarch64).  Colab notebooks reproduce results on a
free T4 GPU.

\subsection{Benchmarks}

\subsubsection{IOI Circuit Recovery}

The Indirect Object Identification task~\cite{wang2022interpretability}
tests whether the agent can identify features causally responsible for
predicting the indirect object.  We use 5 prompts of the form
``When [A] and [B] went to the store, [A] gave the bag to \_\_''
where the correct prediction is [B].  For each prompt, we measure the
KL divergence caused by ablating each candidate feature via the
\texttt{feature\_intervention} API, which correctly intervenes at the
transcoder level with proper network propagation.

\subsubsection{Feature Steering}

Three concept prompts (Golden Gate Bridge, Eiffel Tower, Mount Everest).
For each, we identify the top 10 features by graph importance and scale
their activations at multipliers $m \in \{0, 2, 5, 10\}$.  We measure
the KL divergence and whether the model's top prediction changes.

\subsection{Baselines}

\begin{itemize}
\item \textbf{Random:} Select features uniformly at random (10 trials).
\item \textbf{Greedy:} Select features in descending order of graph
  node influence (sum of absolute adjacency weights).
\item \textbf{Oracle:} Select features in descending order of
  true KL divergence (upper bound requiring all ablations).
\end{itemize}

All methods use the same \texttt{feature\_intervention} API and
KL divergence metric.  Budgets are fixed at $B=20$ interventions
per prompt.

\subsection{Metrics}

\begin{itemize}
\item \textbf{Mean KL per intervention:}  Average KL divergence across
  selected features.  Higher values indicate the method finds more
  causally important features.

\item \textbf{Cumulative KL:}  Total information gained over the
  budget.  Compared against oracle to compute efficiency.

\item \textbf{Oracle efficiency:}  $\text{Cum.~KL} / \text{Oracle~Cum.~KL}
  \times 100\%$.  Measures how close a method is to optimal.

\item \textbf{Prediction change rate:}  Fraction of steering
  experiments where the model's top-1 prediction changes.
\end{itemize}

\subsection{Active Inference Selector}

The \ACD{} selector combines graph-structural importance with
uncertainty-weighted exploration.  For each candidate feature $i$:
\[
  \text{score}(i) = \underbrace{\text{imp}(i) \cdot \lambda_{\ell(i)}}_{\text{pragmatic}}
  + \underbrace{u(i) \cdot \omega_e}_{\text{epistemic}}
\]
where $\text{imp}(i)$ is the normalized graph influence,
$\lambda_{\ell}$ is a learned per-layer prior updated after each
observation, $u(i)$ is the uncertainty (initialized to 1, reduced
after observation), and $\omega_e = 2.0$ is the exploration weight.

After ablating feature $i$ and observing KL divergence:
\begin{enumerate}
\item $u(i) \leftarrow 0$ (observed feature has no remaining uncertainty).
\item Same-layer features: $u(j) \leftarrow 0.7 \cdot u(j)$ (partial
  uncertainty reduction by transfer).
\item Layer prior $\lambda_\ell$ is updated based on the ratio of
  layer-average KL to global-average KL.
\end{enumerate}

\subsection{Reproducibility}

All code is available at
\url{https://github.com/SharathSPhD/ActiveCircuitDiscovery}.
A Colab notebook reproduces the main experiments on a free T4 GPU.
Docker images are provided for the DGX Spark environment.  All
results derive from real model activations via the
\texttt{feature\_intervention} API---no synthetic or fabricated data.
Raw experiment outputs are stored in \texttt{results/} as JSON.
