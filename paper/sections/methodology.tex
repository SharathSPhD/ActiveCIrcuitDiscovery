This section formalises the \ACD{} framework, detailing the attribution
graph backend, the POMDP-based active inference agent, and the
intervention engine.

\subsection{Architecture Overview}

\begin{figure}[t]
\centering
\resizebox{\columnwidth}{!}{%
\input{figures/architecture}%
}
\caption{System architecture of \ACD{}. The \texttt{circuit-tracer}
  pipeline (left) extracts candidate features via EAP and pruning.
  The Active Inference POMDP agent (right) selects interventions by
  minimising Expected Free Energy, updates beliefs from observations,
  and learns its observation model via Dirichlet updates.}
\label{fig:architecture}
\end{figure}

The framework consists of three layers.  The first is the
\emph{attribution graph backend}, in which Anthropic's
\texttt{circuit-tracer} library~\cite{Anthropic2025CT} generates
attribution graphs via Edge Attribution Patching (EAP) with GemmaScope
transcoders for Gemma-2-2B; the graph contains active transcoder
features, an adjacency matrix encoding feature interactions, and
activation values.  The second layer is the \emph{Active Inference
POMDP agent}, a multi-factor POMDP implemented with
\texttt{pymdp}~\cite{Pymdp2022} that maintains a generative model of
the circuit structure, performs variational state inference, evaluates
candidate interventions via Expected Free Energy, and learns its
observation model online from real intervention data.  The third layer
is the \emph{intervention engine}, which executes feature-level
ablations and steering via the \texttt{feature\_intervention} API,
intervening at the transcoder level with proper network propagation
through the underlying
TransformerLens~\cite{nanda2022transformerlens} model.


\subsection{Candidate Feature Extraction}

Given a prompt, the attribution graph backend produces three outputs:
active features $\{(l_i, p_i, f_i)\}$ where $l$ is layer, $p$ is
token position, and $f$ is feature index; an adjacency matrix
$\mat{W} \in \mathbb{R}^{n \times n}$ encoding feature-to-feature
attribution weights; and activation values
$\vect{a} \in \mathbb{R}^n$.

The graph is pruned to retain features with influence above a threshold
(default 80\%).  Each retained feature $i$ receives a normalised
importance score:
\begin{equation}
  \text{imp}(i) = \frac{\sum_j |W_{ij}| + \sum_j |W_{ji}|}
                       {\max_k \left(\sum_j |W_{kj}| + \sum_j |W_{jk}|\right)}
\end{equation}

Candidates are sampled across layers (up to $k$ per layer) to ensure
diversity.


\subsection{POMDP Formulation}

\begin{figure}[t]
\centering
\resizebox{\columnwidth}{!}{%
\input{figures/efe_pipeline}%
}
\caption{POMDP agent pipeline. At each step, the agent infers
  hidden states from observations, evaluates all action--candidate
  pairs via EFE, selects the pair with lowest EFE, executes the
  intervention, and updates its generative model via Dirichlet learning.}
\label{fig:scoring}
\end{figure}

The circuit discovery problem is cast as a discrete POMDP with the
following structure.

\subsubsection{Hidden State Factors}

Three hidden state factors capture distinct aspects of each candidate
feature.  Factor~0 (\emph{feature importance},
$s_0 \in \{\text{negligible}, \text{low}, \text{moderate},
\text{high}\}$, 4~levels) represents the causal contribution of a
feature to the model's output on the current prompt.  Factor~1
(\emph{layer role},
$s_1 \in \{\text{early}, \text{middle}, \text{late}\}$, 3~levels) is
determined by the feature's position within the network, with layers
divided into thirds.  Factor~2 (\emph{causal influence},
$s_2 \in \{\text{weak}, \text{moderate}, \text{strong}\}$, 3~levels)
reflects the feature's degree of causal control over downstream
computation, inferred from intervention effects.

\subsubsection{Observation Modalities}

Three observation modalities provide complementary evidence about
hidden states.  Modality~0 (\emph{KL divergence magnitude}) is the
KL divergence between the clean and intervened output distributions,
discretised into four levels: negligible ($<10^{-4}$), small
($<10^{-3}$), medium ($<10^{-2}$), and large ($\geq 10^{-2}$).
Modality~1 (\emph{activation magnitude}) is the absolute activation
value of the feature, also discretised into four levels.  Modality~2
(\emph{graph connectivity}) is the sum of in-degree and out-degree
of the feature in the attribution graph, discretised into three
levels (sparse, moderate, dense).

\subsubsection{Actions}

Three intervention types are available: \emph{ablation}, which sets
the transcoder feature activation to zero; \emph{activation patching},
which replaces the feature activation with a reference value from a
different prompt; and \emph{feature steering}, which scales the
activation by a multiplier $m \in \{0, 2, 5, 10\}$.

\subsubsection{Generative Model}

The generative model consists of four components, following the
standard Active Inference formulation~\cite{Friston2017,Parr2022}.
The observation likelihood $\mat{A}$ specifies
$P(o_m \mid s_0, s_1, s_2)$ for each modality~$m$, encoding
domain-informed priors about how hidden feature states produce
observable intervention effects; this matrix is learned online via
Dirichlet concentration updates from each observation.  The
transition model $\mat{B}$ specifies $P(s' \mid s, a)$: Factor~0
(importance) is controlled by the three actions, while Factors~1
and~2 have identity transitions reflecting intrinsic properties that
do not change upon intervention.  The preference model $\mat{C}$
is a log-prior over preferred observations in which higher KL
divergence and higher activation are favoured, encoding the goal of
finding causally important features.  Finally, the prior $\mat{D}$
over hidden states is biased toward low importance---since most
features are not causally critical---and uniform over layer role.


\subsection{Intervention Selection via Expected Free Energy}

At each step, the agent evaluates each unobserved candidate feature $i$
and each action $a$ by computing the Expected Free Energy:
\begin{multline}
  G(i, a) = \underbrace{\mathbb{E}_{q}\!\bigl[\log q(s_\tau | \pi) - \log q(s_\tau | o_\tau, \pi)\bigr]}_{\text{epistemic: information gain}} \\
  + \underbrace{\mathbb{E}_{q}\!\bigl[\log p(o_\tau) - \log q(o_\tau | \pi)\bigr]}_{\text{pragmatic: preference satisfaction}}
\end{multline}

The candidate--action pair with the lowest EFE is selected:
\begin{equation}
  (i^*, a^*) = \arg\min_{(i,a)} G(i, a)
\end{equation}

This selection criterion naturally trades off exploration (choosing
features that maximally reduce uncertainty about hidden states) with
exploitation (choosing features that are expected to produce preferred
observations, namely high KL divergence).


\subsection{Belief Updating and Learning}

After executing the selected intervention and observing the resulting
KL divergence, activation magnitude, and graph connectivity, the agent
performs three updates.  First, it infers posterior states: the pymdp
agent performs variational inference to update
$q(s_0, s_1, s_2 \mid o_0, o_1, o_2)$.  Second, it updates the
observation model by incrementing the Dirichlet concentration
parameters:
\begin{equation}
  p_{A_m}(o_m, s_0, s_1, s_2) \mathrel{+}= \eta \cdot q(s_0) \cdot q(s_1) \cdot q(s_2)
\end{equation}
where $\eta$ is the learning rate; this enables the agent to improve
its mapping from hidden states to observations as it accumulates
intervention data.  Third, it advances its internal time index and
resets the observation buffer for the next step.


\subsection{Convergence Detection}

The agent monitors the KL divergence between successive belief
distributions over a rolling window. When the average belief change
falls below a threshold $\theta = 0.01$, the agent signals convergence,
indicating that further interventions are unlikely to substantially
revise the inferred circuit structure.


\subsection{Intervention Engine}

\begin{figure}[t]
\centering
\resizebox{0.85\columnwidth}{!}{%
\input{figures/circuit_flow}%
}
\caption{Flow of a single intervention step. The POMDP agent
  evaluates all candidate--action pairs via EFE, selects the best,
  executes the intervention via \texttt{feature\_intervention}, and
  updates its generative model from the resulting observations.}
\label{fig:flow}
\end{figure}

The intervention engine implements two manipulation types via the
\texttt{feature\_intervention} API:

\textbf{Ablation:} Sets transcoder feature $(l, p, f)$ to zero:
\begin{equation}
  \text{logits}_{\text{abl}} = \text{feature\_intervention}(\text{prompt},
  [(l, p, f, 0)])
\end{equation}

\textbf{Feature Steering:} Scales activation by multiplier $m$:
\begin{equation}
  \text{logits}_{\text{steer}} = \text{feature\_intervention}(\text{prompt},
  [(l, p, f, a_i \cdot m)])
\end{equation}

where $a_i$ is the clean activation value of feature $i$.

Effect is measured via KL divergence between the clean and intervened
output distributions:
\begin{equation}
  \text{KL}_i = D_{\text{KL}}\!\left(
    \text{softmax}(\text{logits}_{\text{interv}})
    \;\|\;
    \text{softmax}(\text{logits}_{\text{clean}})
  \right)
\end{equation}

The \texttt{feature\_intervention} API correctly propagates the
intervention through the replacement model's transcoder structure,
ensuring that the measured effect reflects the true causal contribution
of the feature rather than a residual-stream approximation.
