% ---- discussion.tex ----

\subsection{POMDP Agent vs.\ Heuristic Baselines}

The multi-action POMDP agent, implemented with \texttt{pymdp},
substantially outperforms all baselines on Gemma-2-2B, achieving
1255\% oracle efficiency on IOI and 983\% on multi-step reasoning.
On Llama-3.2-1B, it outperforms baselines on IOI (90.6\% oracle
efficiency) and domain tasks, though the bandit heuristic leads
on multi-step reasoning (76.4\% vs.\ 33.6\%).

Oracle efficiencies exceeding 100\% on Gemma arise from the
multi-action system.  The oracle uses ablation-only KL divergences,
but the POMDP agent can select feature steering interventions that
amplify causal effects and produce higher KL than ablation of the
same feature.  This is a genuine advantage of the multi-action
framework, not a measurement artefact.

On Llama's multi-step reasoning task, the shallower architecture
(16 layers vs.\ 26) compresses the POMDP state space for the
layer-role factor.  With fewer layers providing less distributional
diversity, the simpler bandit heuristic proves more effective.

\subsection{Action Selection Dynamics}

The agent's action distribution over time reveals the
exploration--exploitation dynamics predicted by the EFE framework.
On Gemma IOI, ablation is selected at the first step for its high
transition entropy (maximal information gain), after which the agent
transitions to predominantly feature steering (lowest transition
entropy, highest confirmatory value).  This pattern is consistent
with the symmetric B-matrix design (Appendix~\ref{app:bmatrix}),
where the epistemic term drives action selection based on the
agent's current belief concentration.

Activation patching appears occasionally when the agent's importance
belief is at an intermediate confidence level, consistent with
patching's moderate transition entropy.  The action distribution
(Figure~\ref{fig:action_dist}) visualises this temporal progression.

\subsection{Relationship to Expected Free Energy}

The scoring function used by the POMDP agent is the Expected Free
Energy as defined by Da Costa et al.~\cite{DaCosta2020}, computed
by \texttt{pymdp} over the full generative model. This is distinct
from the lightweight scoring heuristic used in the bandit baseline,
which approximates the exploration--exploitation trade-off through
additive graph importance and uncertainty terms. The agent's
generative model provides a principled probability distribution over
future observations and hidden states, enabling it to reason about
which interventions would be most informative rather than merely
which features have the highest current score.

An important caveat, noted by Tschantz et al.~\cite{Tschantz2020},
is that EFE minimisation reduces to reward maximisation in the limit
of zero epistemic value, and to information gain in the limit of
zero pragmatic value. The present experiments operate in a regime
where both terms contribute, and the relative weighting
($\omega_{\text{epistemic}} = \omega_{\text{pragmatic}} = 1.0$)
was selected without extensive tuning.

\subsection{Task-Dependent Circuit Structure}

A notable finding is the task-dependent layer distribution of causally
important features. IOI circuits concentrate in late layers (24--25),
consistent with prior work identifying name-mover attention heads in
the final transformer layers~\cite{Wang2022}. In contrast, multi-step
reasoning and logic features concentrate in early layers (0--8),
suggesting that transitive inference relies on entity-binding and
input-processing computations before later output selection.

The multi-domain benchmark (\cref{fig:domain_layers}) extends this
finding across five cognitive categories.  Geography and history
prompts show late-layer dominance (consistent with factual recall from
stored knowledge), while mathematics and logic prompts peak in early
layers (consistent with syntactic pattern matching).  Science prompts
show a more uniform distribution, reflecting a mix of factual recall
and compositional reasoning.

This finding has implications for circuit discovery methodology: a
selector that does not explore early layers will miss critical features
for reasoning tasks, while one that does not focus on late layers will
miss output-critical features. The POMDP agent's layer-role state
factor naturally adapts to both patterns.

\subsection{Cross-Model Generality}

Replicating the evaluation on Llama-3.2-1B~\cite{Dubey2024llama}
confirms that the \ACD{} framework is not architecture-specific.
The POMDP agent achieves 90.6\% oracle efficiency on Llama IOI,
outperforming the bandit baseline (88.2\%).  On domain tasks, the
agent achieves improvements exceeding 1000\% over random on Llama
for logic and history domains.  The reduced performance on Llama
multi-step reasoning is expected: fewer layers compress the state
space for the layer-role factor, reducing the scope for the agent's
multi-factor belief model to differentiate layer regions.

\subsection{Limitations}

\textbf{Multi-step reasoning on Llama.}  The POMDP agent
under-performs the bandit on Llama multi-step reasoning (33.6\% vs.\
76.4\% oracle efficiency).  The compressed state space of the 16-layer
architecture provides less distributional diversity for the multi-factor
belief model to exploit.

\textbf{Limited prompt sets.}  The current evaluation uses 5 IOI prompts,
3 multi-step prompts, and 10 multi-domain prompts.  Statistical
significance tests require $n \geq 30$ prompts for reliable $p$-values.

\textbf{Discretisation sensitivity.}  The discretisation thresholds
for observations (KL, activation, connectivity) are calibrated from
the empirical ranges observed in initial experiments.  Different
threshold choices may affect the agent's performance, and an
adaptive binning strategy could improve robustness.

\textbf{Single-step planning.}  The current agent uses a policy
length of 1 (single-step lookahead).  Multi-step planning, where the
agent reasons about sequences of complementary interventions, could
further improve efficiency at the cost of combinatorial complexity.

\subsection{Broader Implications for AI Safety}

Mechanistic interpretability is increasingly recognised as a foundation
for AI safety: if internal computations can be audited as circuits, it
becomes possible to verify that models are not relying on undesirable
reasoning shortcuts~\cite{Hubinger2019,Bereska2024}.  The \ACD{} framework
contributes by providing an efficient, principled discovery tool
that scales to production-size models (2B+ parameters) and generalises
across architectures.  The finding that circuit structure is
task-dependent underscores the need for systematic, multi-benchmark,
multi-model evaluation in any interpretability audit.
