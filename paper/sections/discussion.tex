% ---- discussion.tex ----

\subsection{POMDP Agent vs.\ Heuristic Baselines}

The POMDP agent, implemented with \texttt{pymdp}, consistently
outperforms random selection across all experiments. However, the
bandit heuristic and greedy importance ranking achieve higher oracle
efficiency on IOI (74.4\% vs.\ 58.3\%) and multi-step reasoning
(78.4\% vs.\ 73.3\%).

This result is informative rather than negative. The POMDP agent's
lower oracle efficiency reflects its heavier epistemic exploration:
the agent spends interventions on features that reduce uncertainty
about the circuit structure, even when those features turn out to
have low KL divergence. The bandit, by contrast, concentrates on
features with high expected reward from the start.

At a budget of $B=20$, this exploration cost is not recovered because
the session ends before the learned observation model can be fully
exploited. The POMDP agent's advantage is expected to increase with
larger budgets and across multiple sessions, where the learned
A-matrix can transfer knowledge about layer--importance relationships.
This hypothesis is a target for future work with expanded prompt sets.

\subsection{Relationship to Expected Free Energy}

The scoring function used by the POMDP agent is the Expected Free
Energy as defined by Da Costa et al.~\cite{DaCosta2020}, computed
by \texttt{pymdp} over the full generative model. This is distinct
from the lightweight scoring heuristic used in the bandit baseline,
which approximates the exploration--exploitation trade-off through
additive graph importance and uncertainty terms. The agent's
generative model provides a principled probability distribution over
future observations and hidden states, enabling it to reason about
which interventions would be most informative rather than merely
which features have the highest current score.

An important caveat, noted by Tschantz et al.~\cite{Tschantz2020},
is that EFE minimisation reduces to reward maximisation in the limit
of zero epistemic value, and to information gain in the limit of
zero pragmatic value. The present experiments operate in a regime
where both terms contribute, and the relative weighting
($\omega_{\text{epistemic}} = \omega_{\text{pragmatic}} = 1.0$)
was selected without extensive tuning.

\subsection{Task-Dependent Circuit Structure}

A notable finding is the task-dependent layer distribution of causally
important features. IOI circuits concentrate in late layers (24--25),
consistent with prior work identifying name-mover attention heads in
the final transformer layers~\cite{Wang2022}. In contrast, multi-step
reasoning and logic features concentrate in early layers (0--8),
suggesting that transitive inference relies on entity-binding and
input-processing computations before later output selection.

The multi-domain benchmark (\cref{fig:domain_layers}) extends this
finding across five cognitive categories.  Geography and history
prompts show late-layer dominance (consistent with factual recall from
stored knowledge), while mathematics and logic prompts peak in early
layers (consistent with syntactic pattern matching).  Science prompts
show a more uniform distribution, reflecting a mix of factual recall
and compositional reasoning.

This finding has implications for circuit discovery methodology: a
selector that does not explore early layers will miss critical features
for reasoning tasks, while one that does not focus on late layers will
miss output-critical features. The POMDP agent's layer-role state
factor naturally adapts to both patterns.

\subsection{Cross-Model Generality}

Replicating the evaluation on Llama-3.2-1B~\cite{Dubey2024llama}
confirms that the \ACD{} framework is not architecture-specific.
Despite Llama's smaller depth (16 layers vs.\ 26), the POMDP agent
achieves competitive oracle efficiency with improvement over random,
compared to higher performance on Gemma.  The modest reduction is
expected: fewer layers compress the state space for the layer-role
factor, reducing the scope for the agent's multi-factor belief
model to differentiate layer regions.

\subsection{Limitations}

\textbf{Greedy parity on concentrated circuits.}  When causally important
features are concentrated in a few layers (as in multi-step reasoning),
the POMDP agent performs comparably to greedy importance ranking. The
epistemic component provides less differentiation when there is limited
distributional diversity across the state space.

\textbf{Limited prompt sets.}  The current evaluation uses 5 IOI prompts,
3 multi-step prompts, and 10 multi-domain prompts.  Statistical
significance tests require $n \geq 30$ prompts for reliable $p$-values.

\textbf{Discretisation sensitivity.}  The discretisation thresholds
for observations (KL, activation, connectivity) are calibrated from
the empirical ranges observed in initial experiments.  Different
threshold choices may affect the agent's performance, and an
adaptive binning strategy could improve robustness.

\textbf{Single-step planning.}  The current agent uses a policy
length of 1 (single-step lookahead).  Multi-step planning, where the
agent reasons about sequences of complementary interventions, could
further improve efficiency at the cost of combinatorial complexity.

\subsection{Broader Implications for AI Safety}

Mechanistic interpretability is increasingly recognised as a foundation
for AI safety: if internal computations can be audited as circuits, it
becomes possible to verify that models are not relying on undesirable
reasoning shortcuts~\cite{Hubinger2019,Bereska2024}.  The \ACD{} framework
contributes by providing an efficient, principled discovery tool
that scales to production-size models (2B+ parameters) and generalises
across architectures.  The finding that circuit structure is
task-dependent underscores the need for systematic, multi-benchmark,
multi-model evaluation in any interpretability audit.
