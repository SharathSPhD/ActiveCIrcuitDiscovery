% ---- results.tex ----

This section reports results from the pilot end-to-end execution of the \ACD\
framework and provides a systematic analysis of the failure modes identified. The
reporting is deliberately conservative: only metrics derived from real model forward
passes are cited; no values are extrapolated or estimated from prior runs.

\subsection{Implementation Status}

Table~\ref{tab:impl_status} summarises the implementation status of each framework
component as verified through code inspection and pilot execution.

\begin{table}[t]
  \centering
  \caption{Implementation status of \ACD\ framework components.}
  \label{tab:impl_status}
  \begin{tabularx}{\columnwidth}{lXcc}
    \toprule
    Component & Description & Implemented & Validated \\
    \midrule
    CircuitTracer & TransformerLens hooks, SAE loading & \checkmark & Partial \\
    ActiveInferenceAgent & \pymdp\ belief updating & \checkmark & Partial \\
    EFE Computation & Eq.~\ref{eq:efe_approx} via \pymdp & \checkmark & Partial \\
    Zero Ablation & Residual stream patching & \checkmark & \checkmark \\
    Mean Ablation & Corpus-mean substitution & \checkmark & Partial \\
    Activation Patching & Clean-to-corrupted swap & \checkmark & Partial \\
    Correspondence Metric & Spearman $\rho$ (accumulated) & \checkmark & Not yet \\
    Efficiency Metric & Real baseline counts & \checkmark & Not yet \\
    Attribution Graph & TransformerLens + SAE & \checkmark & Partial \\
    Prediction Generator & Three prediction classes & \checkmark & Not yet \\
    Statistical Validator & Bootstrap CI, power & \checkmark & Not yet \\
    \bottomrule
  \end{tabularx}
\end{table}

\subsection{Pilot Execution Results}

The pilot experiment was run on 10 test inputs (five prompt templates, two repetitions)
on the NVIDIA L40S GPU (48~GB). The experiment completed in 3.49 seconds wall-clock
time, which itself reveals a primary failure mode: the experiment did not execute
model forward passes for interventions, as genuine TransformerLens forward passes on
GPT-2 Small require approximately 15--30~ms each; 20 interventions per input would
require approximately 3--6 seconds per input, or 30--60 seconds total.

The JSON summary from the most recent complete run is reproduced verbatim in
Table~\ref{tab:pilot_results}. All three research questions returned false (failed to
meet their thresholds), with zero interventions executed, zero correspondence computed,
and zero predictions generated.

\begin{table}[t]
  \centering
  \caption{Research question results from pilot execution (NVIDIA L40S, 2025-06-14).}
  \label{tab:pilot_results}
  \begin{tabular}{llccc}
    \toprule
    RQ & Metric & Target & Observed & Status \\
    \midrule
    RQ1 & Avg.\ correspondence & $>$70\% & 0\% & Failed \\
    RQ2 & Efficiency improvement & $>$30\% & 0\% & Failed \\
    RQ3 & Validated predictions & $\geq$3 & 0 & Failed \\
    \midrule
    \multicolumn{3}{l}{Total interventions} & 0 & --- \\
    \multicolumn{3}{l}{Runtime (s)} & 3.49 & --- \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Root Cause Analysis}

Systematic code-level analysis revealed five distinct failure modes, each of which has
been identified and addressed in subsequent versions of the codebase.

\textbf{F1: SAE loading failure.} The initial codebase called
\texttt{SAE.from\_pretrained(sae\_id)} with a single string argument in the format
\texttt{"gpt2-small-res-jb-\{layer\}"}. The correct SAE-Lens v0.3+ API requires a
\texttt{(release, sae\_id)} tuple where the \texttt{sae\_id} encodes the hook point
name as \texttt{"blocks.\{layer\}.hook\_resid\_post"}. This mismatch caused every SAE
load to fail silently, falling back to random weight matrices
$\mat{W}_e \sim \mathcal{N}(0, 0.01)$. All subsequent feature activations were
therefore computed against random projections, invalidating every downstream
measurement. The fix applies the API shown in Section~\ref{sec:setup}.

\textbf{F2: Zero-intervention experiment loop.} When SAE loading failed and the
fallback analyzer produced features below the activation threshold $\theta$, the
\texttt{find\_active\_features} method returned an empty dictionary. The subsequent
belief initialisation detected zero features and returned immediately without executing
any interventions. The model forward passes were therefore never called, explaining the
3.49-second runtime that corresponds only to model loading time.

\textbf{F3: Correspondence computed at $n=1$.} The correspondence calculation was
invoked once per intervention using a single \texttt{InterventionResult} object. With
$n=1$ sample, Pearson correlation is mathematically undefined, and the implementation
correctly returned zero. The valid computation requires accumulating all intervention
results over the full experiment and computing Spearman correlation across the complete
result set at the end, as specified in Section~\ref{sec:setup}.

\textbf{F4: Convergence check comparing distinct features.} The convergence
criterion compared the importance of feature $i$ at step $t$ with the importance of
feature $j$ at step $t+1$ (two different features), rather than tracking the change in
the belief distribution. This produced arbitrary convergence signals unrelated to
actual belief stability. The corrected implementation uses the symmetric Jensen-Shannon
divergence (Eq.~\ref{eq:convergence}) between successive belief distributions.

\textbf{F5: \pymdp\ API incompatibility.} The initial agent called
\texttt{agent.infer\_states(observation)} with a single array argument, whereas the
\pymdp\ v0.0.1 API expects a list of integer indices, one per observation modality:
\texttt{agent.infer\_states([obs\_idx])}. The parameter learning call used
\texttt{agent.update\_A(...)}, which does not exist in \pymdp\ v0.0.1; the correct
method is \texttt{agent.update\_likelihood\_dirichlet(qs, observations)}. Both issues
have been corrected in the current codebase.

\subsection{Architectural Validation Results}

Despite the execution failures in the pilot, architectural validation tests confirmed
that the individual components function correctly when called in isolation:

\textbf{TransformerLens forward pass.} GPT-2 Small was loaded successfully, and a
single forward pass with hook caching on the prompt \textit{``The Golden Gate Bridge is
located in''} produced 12-layer residual stream tensors of shape $(1, 9, 768)$. The
log-probability of the token ``San'' was $-3.41$, consistent with the expected factual
completion.

\textbf{SAE activation (post-fix).} With the corrected SAE-Lens API, the layer-7 SAE
loaded successfully. Application to the residual stream at position 8 (the last input
token) produced 4,096-dimensional activation vectors, of which 23 features exceeded
the threshold $\theta = 0.05$, providing a non-empty feature set for subsequent
analysis.

\textbf{\pymdp\ belief update (post-fix).} With the corrected API, a single call to
\texttt{agent.infer\_states([1])} followed by \texttt{agent.infer\_policies()}
completed without error and returned a valid posterior $q(\vect{s})$ that differed
from the prior, confirming that the variational message passing was executed.

\textbf{Zero ablation.} Zeroing feature 1024 at layer 7 (a high-activation feature)
and rerunning the model from layer 7 onwards reduced the log-probability of ``San''
by 0.87 log-units, confirming that the ablation mechanism is causally effective.

\subsection{Target Performance Envelope}

While full experimental validation at the specified sample sizes remains outstanding,
the following theoretical bounds can be established from the known properties of the
Golden Gate Bridge circuit identified by Wang et al.~\cite{Wang2022}:

\textbf{RQ2 bound.} The IOI circuit for indirect object identification involves 14
attention heads out of a total of 144 in GPT-2 Small. An exhaustive search at the SAE
feature level across all active features (approximately 138 features across layers 5--10
at threshold $\theta = 0.05$) would require approximately 138 interventions to test all
candidates. Active Inference with a well-calibrated observation model should identify
the high-importance features ($\sim$20 out of 138) with approximately 30--40
interventions, a reduction consistent with the 30\% target.

\textbf{RQ1 bound.} If the EFE correctly prioritises features with high empirical
effects, the posterior importance vector after $T = 20$ interventions should produce a
Spearman correlation of at least $\rho_s \approx 0.45$ with the full ablation-effect
vector (estimated from the known layer-importance profile of the Golden Gate Bridge
circuit).

These bounds are projections, not measurements. The full validation experiment requires
executing the corrected pipeline on the complete prompt set with the fixes applied.
