% ---- introduction.tex ----

The rapid growth in the scale and deployment of large language models (LLMs) has
intensified the need for principled methods to audit, explain, and predict their
behaviour~\cite{Bereska2024}. Mechanistic interpretability pursues this goal by
reverse-engineering the internal computations of trained models into human-legible
circuits~\cite{Olah2020,Elhage2021}. A circuit, in this context, denotes the minimal
subgraph of model components (attention heads, MLP neurons, or SAE features) whose
activations are jointly necessary and sufficient to reproduce a specific model
behaviour on a given family of inputs~\cite{Cammarata2020,Wang2022}.

Circuit analysis has produced landmark findings. Wang et al.\ identified a
fourteen-head circuit mediating Indirect Object Identification (IOI) in GPT-2
Small~\cite{Wang2022}; Hanna et al.\ localised a circuit for numerical
greater-than comparisons~\cite{Hanna2023}; and Nanda et al.\ traced the emergence of
modular arithmetic generalisation through a Fourier-space circuit during
grokking~\cite{Nanda2023Grokking}. Each of these studies required thousands of
manually guided interventions, prompting interest in automation~\cite{Conmy2023,Syed2023}.

Automated Circuit Discovery (\ACDC)~\cite{Conmy2023} generalises the activation
patching methodology of Wang et al.\ to a greedy graph-pruning algorithm that
systematically tests every edge in the computational graph. Although \ACDC\ achieves
strong fidelity on the IOI task, the number of interventions scales as $O(E)$ in the
number of graph edges, which for deep transformer models reaches the tens of
thousands. Edge Attribution Patching (\EAP)~\cite{Syed2023} reduces this cost by
approximating patch effects with integrated gradients, yet it trades correctness for
efficiency and may miss higher-order interactions. Neither method incorporates
uncertainty about the circuit structure or adapts its search strategy based on what
has already been discovered.

Active Inference is a normative framework for perception and action in biological
agents that unifies perception, learning, and planning under a single objective: the
minimisation of variational free energy (equivalently, the maximisation of model
evidence)~\cite{Friston2010,Parr2022}. When extended to planning, the framework
introduces the Expected Free Energy (\EFE), which decomposes into an epistemic term
(expected information gain about hidden states) and a pragmatic term (expected
alignment with preferences)~\cite{Friston2015,DaCosta2020}. \EFE\
minimisation naturally trades off exploration and exploitation: an agent selects
actions that resolve uncertainty about the world while pursuing desired outcomes.

This work proposes that circuit discovery is precisely the kind of problem for which
\EFE\ minimisation is appropriate. The ``world'' is the causal graph of an LLM; the
``actions'' are ablation and activation-patching interventions; the ``hidden states''
are the identities and importances of circuit-critical features; and the ``preference''
is the rapid identification of a faithful, minimal circuit. An Active Inference agent
that maintains a belief distribution over candidate circuit components and selects
interventions to maximally resolve that uncertainty will, in theory, identify circuits
with fewer total interventions than either random or gradient-ranked selection.

The contributions of this paper are as follows.

\textbf{(C1) Active Inference formulation.} Circuit discovery is cast as a
partially observable Markov decision process (POMDP) with three hidden state
factors (feature importance, layer role, causal influence), three observation
modalities (KL divergence, activation magnitude, graph connectivity), and three
actions (ablation, patching, steering). The agent is implemented using the
\texttt{pymdp} library~\cite{Pymdp2022}, with Expected Free Energy guiding
intervention selection and Dirichlet parameter updates enabling online learning
of the observation model.

\textbf{(C2) Framework implementation.} The Active Circuit Discovery (\ACD)
framework integrates Anthropic's
\texttt{circuit-tracer}~\cite{Anthropic2025CT,Ameisen2025} for Edge Attribution Patching
with GemmaScope transcoders and the \texttt{feature\_intervention} API for
causally correct transcoder-level interventions.

\textbf{(C3) Validated evaluation.} The POMDP agent is compared against a
heuristic bandit baseline, greedy importance ranking, random selection, and an
oracle upper bound on two models (Gemma-2-2B and Llama-3.2-1B) across four
benchmarks (IOI, multi-step reasoning, feature steering, and a five-domain
cognitive benchmark).

\textbf{(C4) Causal controllability.} Feature steering experiments demonstrate
that individual transcoder features causally control model behaviour, with
prediction changes observed at $10\times$ activation scaling.

\textbf{(C5) Reproducibility artifacts.} Three Google Colab notebooks, a Docker
container for NVIDIA DGX Spark, and raw experiment results (JSON) are released
for full reproduction.

The following testable hypotheses guide the evaluation.

\textbf{H1 (Efficiency).}  The POMDP agent identifies causally important
features with higher cumulative KL divergence than random selection within
a fixed budget of $B=20$ interventions. Assessed via a one-sided paired
$t$-test across prompts ($\alpha = 0.05$).

\textbf{H2 (Causal controllability).}  Scaling individual transcoder features
at $10\times$ activation changes the model's top-1 prediction for a
non-trivial fraction of features. Assessed via a one-sided binomial test
against a 1\% chance-level baseline ($\alpha = 0.05$).

\textbf{H3 (Discovery quality).}  The POMDP agent achieves oracle
efficiency $\geq 50\%$ with budget $B=20$. Assessed by point estimate and
standard deviation across prompts.

\textbf{H4 (Cross-architecture transfer).}  The qualitative findings
obtained on Gemma-2-2B replicate on Llama-3.2-1B.

The remainder of this paper is structured as follows. Section~\ref{sec:background}
reviews related work on circuit analysis, Active Inference, and sparse
autoencoders. Section~\ref{sec:method} describes the \ACD{} framework in detail.
Section~\ref{sec:setup} specifies the experimental protocol. Section~\ref{sec:results}
reports validated results. Section~\ref{sec:conclusion} concludes.
