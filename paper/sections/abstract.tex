Mechanistic interpretability seeks to reverse-engineer the computational
circuits within large language models (LLMs), yet current methods rely on
exhaustive or heuristic search over exponentially many feature
interactions. We introduce \emph{Active Circuit Discovery} (\ACD{}), a
framework that combines attribution graph analysis with active
inference--inspired feature selection for efficient circuit discovery.
Our approach integrates Anthropic's \texttt{circuit-tracer} library as
the attribution graph backend, using Edge Attribution Patching with
GemmaScope transcoders to identify ${\sim}12{,}000$ active transcoder
features per prompt in Gemma-2-2B. An uncertainty-weighted selector then
balances exploitation (graph-structural importance) with exploration
(per-feature uncertainty reduction) to prioritise the most informative
feature-level interventions. Across two benchmarks---Indirect Object Identification (IOI, 5 prompts)
and multi-step reasoning (3 prompts)---with a budget of 20 ablation
interventions per prompt, \ACD{} identifies causally important features
36--44\% faster than random selection and achieves 74--78\% of
oracle-optimal cumulative information gain.
Feature steering experiments confirm causal controllability: scaling
individual transcoder features at $10\times$ activation changes model
predictions for 20\% of tested features (KL divergences up to 1.06).
Multi-step reasoning reveals that causally important features concentrate
in early layers (0--8), contrasting with IOI circuits that peak in late
layers (24--25).
All experiments use real model activations via the
\texttt{feature\_intervention} API with no synthetic or fabricated data.
Code, notebooks, and experiment results are publicly available.
