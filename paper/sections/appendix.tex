% ---- appendix.tex ----

\section{Active Inference Selector Parameters}
\label{app:selector}

This appendix details the initialisation and hyperparameters of the
Active Inference Selector.

\textbf{Exploration weight ($\omega_e$).}  Default value: 2.0.  Higher
values increase early-stage exploration of uncertain features at the
cost of potentially delaying exploitation of known high-value features.
The value was selected based on the IOI benchmark to balance oracle
efficiency with baseline improvement.

\textbf{Uncertainty initialisation.}  All features start with $u(i) = 1$.
After observing feature $i$, its uncertainty drops to 0.  Same-layer
features receive a 30\% reduction ($u(j) \leftarrow 0.7 \cdot u(j)$),
and adjacent-position features receive a 10\% reduction
($u(j) \leftarrow 0.9 \cdot u(j)$).

\textbf{Layer prior.}  Initialised to $\lambda_\ell = 1$ for all layers.
Updated after each observation as
$\lambda_\ell = 1 + 0.5 (\bar{\text{KL}}_\ell / \bar{\text{KL}}_{\text{global}} - 1)$.

\textbf{Prior observation derivation.}  Before an intervention is
executed, the agent derives a prior observation for each candidate from
its graph metadata.  The normalised graph importance $\text{imp}(i)
\in [0,1]$ is scaled by a factor of $0.01$ before discretisation:
$o_{\text{prior}} = \text{discretise}(\text{imp}(i) \times 0.01)$.
This maps the graph-derived importance into the KL divergence
threshold range $[10^{-4}, 10^{-2}]$, ensuring that the prior
observation reflects expected KL magnitudes rather than raw graph
weights.

\textbf{Candidate extraction.}  Up to 5 features per layer, 60 total
candidates, selected from pruned graph features sorted by influence
(sum of absolute adjacency weights).  Pruning thresholds: node = 0.8,
edge = 0.98 (defaults from \texttt{circuit-tracer}).


\section{B-Matrix Transition Priors}
\label{app:bmatrix}

The transition model $\mat{B}$ for Factor~0 (feature importance) uses
action-conditioned dynamics that encode the causal semantics of each
intervention type.  These are calibrated priors informed by the
distinct informational roles of ablation, activation patching, and
feature steering.

\textbf{Ablation (action~0)} is an exploratory intervention that
removes a feature entirely, producing the broadest distribution of
possible outcomes.  Its transition matrix has the lowest diagonal
concentration (50\% stay, 25\% down, 25\% up), yielding the highest
transition entropy among the three actions.  Under the EFE
decomposition, this high entropy translates into high expected
information gain, making ablation the preferred action when the
agent is uncertain about a feature's importance.

\textbf{Activation patching (action~1)} replaces the feature
activation with a reference value, providing a moderately
informative signal.  The transition is more concentrated
(70\% stay, 15\% down, 15\% up), producing moderate information
gain.  Patching is selected when the agent has partial evidence
and seeks to refine its importance estimate.

\textbf{Feature steering (action~2)} scales the feature activation
by a multiplier while preserving its directional contribution.
This near-identity transition (90\% stay, 5\% down, 5\% up) has
the lowest transition entropy, making it the preferred action
when the agent's belief is already concentrated and confirmation
is sought rather than exploration.

All transition probabilities are symmetric around the current state
to ensure that the expected utility is similar across actions, so
that the epistemic term (information gain) drives action selection.
These probabilities are fixed hyperparameters, not learned from data.
Varying the diagonal element of the ablation transition matrix between
0.4 and 0.7 did not qualitatively change the action-type distribution
or oracle efficiency rankings across benchmarks; however, a full
hyperparameter sweep is left to future work.


\section{Correspondence Metric}
\label{app:correspondence}

The primary metric for evaluating selector quality is oracle efficiency:
the ratio of cumulative KL divergence achieved by the selector to that
achieved by the oracle (features sorted by true KL divergence,
descending):
\begin{equation}
  \text{Oracle Efficiency} = \frac{\sum_{t=1}^{B} \text{KL}_{i_t^{\text{method}}}}
                                  {\sum_{t=1}^{B} \text{KL}_{i_t^{\text{oracle}}}}
                             \times 100\%
\end{equation}

Secondary metrics include mean KL per intervention (higher = better
feature selection) and improvement percentages over random and greedy
baselines.

KL divergence between clean and intervened output distributions is
computed as:\footnote{The implementation calls
\texttt{kl\_div(log(p\_interv), p\_clean)}, which in PyTorch's
convention yields $D_{\text{KL}}(p_{\text{clean}} \| p_{\text{interv}})$.
This direction measures the information lost when the intervened
distribution is used to approximate the clean one, and is the natural
choice for quantifying the causal impact of an ablation.}
\begin{equation}
  D_{\text{KL}}(p_{\text{clean}} \| p_{\text{interv}}) =
  \sum_v p_{\text{clean}}(v) \log \frac{p_{\text{clean}}(v)}{p_{\text{interv}}(v)}
\end{equation}
using \texttt{torch.nn.functional.kl\_div} with a $10^{-10}$ smoothing
factor to prevent numerical instability.


\section{Docker and DGX Spark Deployment}
\label{app:docker}

The \ACD{} framework is packaged as a Docker container targeting the
NVIDIA DGX Spark platform (GB10 GPU, 128~GB unified memory, CUDA~12.8,
aarch64 architecture).

\begin{lstlisting}[language=bash,caption={Docker build and run for DGX Spark.},breaklines=true]
# Build
docker compose build active-circuit-discovery
# Run experiments
docker compose run active-circuit-discovery \
  python -m src.experiments.run_real_experiments
# Results saved to results/
\end{lstlisting}

The \texttt{docker-compose.yml} in the repository root provides volume
mounts for model weights (cached from HuggingFace Hub) and result
directories.  The container installs \texttt{circuit-tracer} from source
and pins all dependencies via \texttt{requirements.txt}.


\section{Code-Level Fixes Applied}
\label{app:fixes}

Table~\ref{tab:fixes} summarises the critical API fixes discovered
during the implementation and validation process.

\begin{table*}[t]
  \centering
  \caption{Summary of critical API fixes applied to the \ACD{} codebase.}
  \label{tab:fixes}
  \footnotesize
  \newcolumntype{L}{>{\raggedright\arraybackslash}X}
  \begin{tabularx}{\textwidth}{l l L L}
    \toprule
    \textbf{File} & \textbf{Issue} & \textbf{Error} & \textbf{Fix} \\
    \midrule
    \texttt{ct\_backend.py} & LogitTarget attr.\ &
      \texttt{.token} does not exist on \texttt{LogitTarget} &
      Changed to \texttt{.token\_str} \\
    \texttt{ct\_backend.py} & PruneResult &
      Assumed \texttt{prune\_result.graph} exists &
      Apply masks to raw adjacency matrix \\
    \texttt{interv\_engine.py} & Model access &
      \texttt{model.model.cfg} fails &
      Use \texttt{model.cfg} directly \\
    \texttt{interv\_engine.py} & Intervention &
      Used \texttt{run\_with\_hooks} (residual stream) &
      Use \texttt{feature\_intervention} API \\
    \texttt{runner.py} & Fake data &
      \texttt{np.random.beta/normal} for validation &
      Real measurements only \\
    \texttt{pomdp\_agent.py} & Skip bias &
      ``skip'' action 90\% of the time &
      pymdp EFE-based multi-action selection with online Dirichlet learning \\
    \bottomrule
  \end{tabularx}
\end{table*}


\section{Experimental Reproducibility Checklist}
\label{app:repro}

\textbf{Model and data availability:} Gemma-2-2B is available from
HuggingFace Hub (\texttt{google/gemma-2-2b}) under the Gemma license.
GemmaScope transcoders are loaded automatically by \texttt{circuit-tracer}.

\textbf{Hardware requirements:} A single NVIDIA GPU with at least 8~GB
VRAM (tested on T4 with 16~GB in Colab and GB10 with 128~GB on DGX Spark).
The model loads in float32 and requires approximately 5~GB VRAM.

\textbf{Software environment:} All dependencies are pinned in
\texttt{requirements.txt}.  Key packages: \texttt{circuit-tracer}
(from git), \texttt{transformer-lens}, \texttt{torch>=2.0}.

\textbf{Random seed:} Random baselines use
\texttt{numpy.random.seed(42)}.  The Active Inference Selector is
deterministic given the same candidates and exploration weight.

\textbf{Expected runtime:} Approximately 40--60 seconds per prompt on
a single GPU (attribution graph generation: $\sim$18s; 40 ablations at
$\sim$30ms each: $\sim$1.2s; overhead: $\sim$20s for model setup on
first prompt).

\textbf{Raw results:} All experiment outputs are saved as JSON in the
\texttt{results/} directory, including per-prompt KL values for all
methods, feature IDs, layer distributions, and steering outcomes.
