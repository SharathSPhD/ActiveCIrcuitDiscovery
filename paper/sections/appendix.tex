% ---- appendix.tex ----

\section{Active Inference Selector Parameters}
\label{app:selector}

This appendix details the initialisation and hyperparameters of the
Active Inference Selector.

\textbf{Exploration weight ($\omega_e$).}  Default value: 2.0.  Higher
values increase early-stage exploration of uncertain features at the
cost of potentially delaying exploitation of known high-value features.
The value was selected based on the IOI benchmark to balance oracle
efficiency with baseline improvement.

\textbf{Uncertainty initialisation.}  All features start with $u(i) = 1$.
After observing feature $i$, its uncertainty drops to 0.  Same-layer
features receive a 30\% reduction ($u(j) \leftarrow 0.7 \cdot u(j)$),
and adjacent-position features receive a 10\% reduction
($u(j) \leftarrow 0.9 \cdot u(j)$).

\textbf{Layer prior.}  Initialised to $\lambda_\ell = 1$ for all layers.
Updated after each observation as
$\lambda_\ell = 1 + 0.5 (\bar{\text{KL}}_\ell / \bar{\text{KL}}_{\text{global}} - 1)$.

\textbf{Prior observation derivation.}  Before an intervention is
executed, the agent derives a prior observation for each candidate from
its graph metadata.  The normalised graph importance $\text{imp}(i)
\in [0,1]$ is scaled by a factor of $0.01$ before discretisation:
$o_{\text{prior}} = \text{discretise}(\text{imp}(i) \times 0.01)$.
This maps the graph-derived importance into the KL divergence
threshold range $[10^{-4}, 10^{-2}]$, ensuring that the prior
observation reflects expected KL magnitudes rather than raw graph
weights.

\textbf{Candidate extraction.}  Up to 5 features per layer, 60 total
candidates, selected from pruned graph features sorted by influence
(sum of absolute adjacency weights).  Pruning thresholds: node = 0.8,
edge = 0.98 (defaults from \texttt{circuit-tracer}).


\section{B-Matrix Transition Priors}
\label{app:bmatrix}

The transition model $\mat{B}$ for Factor~0 (feature importance) uses
action-conditioned dynamics that encode the causal semantics of each
intervention type.  These are calibrated priors informed by the
distinct informational roles of ablation, activation patching, and
feature steering.

\textbf{Ablation (action~0)} is an exploratory intervention that
removes a feature entirely, producing the broadest distribution of
possible outcomes.  Its transition matrix has the lowest diagonal
concentration (50\% stay, 25\% down, 25\% up), yielding the highest
transition entropy among the three actions.  Under the EFE
decomposition, this high entropy translates into high expected
information gain, making ablation the preferred action when the
agent is uncertain about a feature's importance.

\textbf{Activation patching (action~1)} replaces the feature
activation with a reference value, providing a moderately
informative signal.  The transition is more concentrated
(70\% stay, 15\% down, 15\% up), producing moderate information
gain.  Patching is selected when the agent has partial evidence
and seeks to refine its importance estimate.

\textbf{Feature steering (action~2)} scales the feature activation
by a multiplier while preserving its directional contribution.
This near-identity transition (90\% stay, 5\% down, 5\% up) has
the lowest transition entropy, making it the preferred action
when the agent's belief is already concentrated and confirmation
is sought rather than exploration.

All transition probabilities are symmetric around the current state
to ensure that the expected utility is similar across actions, so
that the epistemic term (information gain) drives action selection.
These probabilities are fixed hyperparameters, not learned from data.
Varying the diagonal element of the ablation transition matrix between
0.4 and 0.7 did not qualitatively change the action-type distribution
or oracle efficiency rankings across benchmarks; however, a full
hyperparameter sweep is left to future work.


\section{Correspondence Metric}
\label{app:correspondence}

The primary metric for evaluating selector quality is oracle efficiency:
the ratio of cumulative KL divergence achieved by the selector to that
achieved by the oracle (features sorted by true KL divergence,
descending):
\begin{equation}
  \text{Oracle Efficiency} = \frac{\sum_{t=1}^{B} \text{KL}_{i_t^{\text{method}}}}
                                  {\sum_{t=1}^{B} \text{KL}_{i_t^{\text{oracle}}}}
                             \times 100\%
\end{equation}

Secondary metrics include mean KL per intervention (higher = better
feature selection) and improvement percentages over random and greedy
baselines.

KL divergence between clean and intervened output distributions is
computed as:\footnote{The implementation calls
\texttt{kl\_div(log(p\_interv), p\_clean)}, which in PyTorch's
convention yields $D_{\text{KL}}(p_{\text{clean}} \| p_{\text{interv}})$.
This direction measures the information lost when the intervened
distribution is used to approximate the clean one, and is the natural
choice for quantifying the causal impact of an ablation.}
\begin{equation}
  D_{\text{KL}}(p_{\text{clean}} \| p_{\text{interv}}) =
  \sum_v p_{\text{clean}}(v) \log \frac{p_{\text{clean}}(v)}{p_{\text{interv}}(v)}
\end{equation}
using \texttt{torch.nn.functional.kl\_div} with a $10^{-10}$ smoothing
factor to prevent numerical instability.


\section{Docker and DGX Spark Deployment}
\label{app:docker}

The \ACD{} framework is packaged as a Docker container targeting the
NVIDIA DGX Spark platform (GB10 GPU, 128~GB unified memory, CUDA~12.8,
aarch64 architecture).

\begin{lstlisting}[language=bash,caption={Docker build and run for DGX Spark.},breaklines=true]
# Build
docker compose build active-circuit-discovery
# Run experiments
docker compose run active-circuit-discovery \
  python -m src.experiments.run_real_experiments
# Results saved to results/
\end{lstlisting}

The \texttt{docker-compose.yml} in the repository root provides volume
mounts for model weights (cached from HuggingFace Hub) and result
directories.  The container installs \texttt{circuit-tracer} from source
and pins all dependencies via \texttt{requirements.txt}.


\section{Experimental Prompts}
\label{app:prompts}

All prompts used in the experiments are listed below.  These are
defined in \texttt{src/experiments/run\_real\_experiments.py} and
correspond exactly to the prompts whose results appear in the JSON
outputs under \texttt{results/}.

\subsection*{IOI Circuit Recovery (5 prompts)}

\begin{enumerate}
  \item ``When John and Mary went to the store, John gave the bag to''
  \item ``After Alice and Bob finished lunch, Alice handed the receipt to''
  \item ``While Sarah and Tom were at the park, Sarah threw the ball to''
  \item ``When Emma and David arrived at the office, Emma passed the keys to''
  \item ``As Lisa and Mike left the restaurant, Lisa returned the coat to''
\end{enumerate}

\subsection*{Multi-step Reasoning (3 prompts)}

\begin{enumerate}
  \item ``If Alice is taller than Bob, and Bob is taller than Carol, then the tallest person is''
  \item ``The capital of France is Paris. Paris is in Europe. The continent containing Paris is''
  \item ``All dogs are animals. Fido is a dog. Therefore Fido is''
\end{enumerate}

\subsection*{Feature Steering (5 concept prompts)}

\begin{enumerate}
  \item \textbf{Golden Gate Bridge}: ``The Golden Gate Bridge is''
  \item \textbf{Eiffel Tower}: ``The Eiffel Tower is located in''
  \item \textbf{Mount Everest}: ``Mount Everest is the tallest''
  \item \textbf{Great Wall of China}: ``The Great Wall of China was built''
  \item \textbf{Statue of Liberty}: ``The Statue of Liberty stands in''
\end{enumerate}

\subsection*{Multi-Domain Benchmark (10 prompts, 2 per domain)}

\textbf{Geography:}
\begin{enumerate}
  \item ``The capital of France is''
  \item ``The Golden Gate Bridge connects San Francisco to''
\end{enumerate}

\textbf{Mathematics:}
\begin{enumerate}
  \item ``The square root of 64 is''
  \item ``If 2 + 3 = 5 then 3 + 4 =''
\end{enumerate}

\textbf{Science:}
\begin{enumerate}
  \item ``Water is made of hydrogen and''
  \item ``The speed of light is approximately''
\end{enumerate}

\textbf{Logic:}
\begin{enumerate}
  \item ``All mammals are warm-blooded. A whale is a mammal. Therefore a whale is''
  \item ``All birds have wings. A penguin is a bird. Therefore a penguin has''
\end{enumerate}

\textbf{History:}
\begin{enumerate}
  \item ``The year World War II ended was''
  \item ``The first person to walk on the moon was''
\end{enumerate}


\section{Experimental Reproducibility Checklist}
\label{app:repro}

\textbf{Model and data availability:} Gemma-2-2B is available from
HuggingFace Hub (\texttt{google/gemma-2-2b}) under the Gemma license.
GemmaScope transcoders are loaded automatically by \texttt{circuit-tracer}.

\textbf{Hardware requirements:} A single NVIDIA GPU with at least 8~GB
VRAM (tested on T4 with 16~GB in Colab and GB10 with 128~GB on DGX Spark).
The model loads in float32 and requires approximately 5~GB VRAM.

\textbf{Software environment:} All dependencies are pinned in
\texttt{requirements.txt}.  Key packages: \texttt{circuit-tracer}
(from git), \texttt{transformer-lens}, \texttt{torch>=2.0}.

\textbf{Random seed:} Random baselines use
\texttt{numpy.random.seed(42)}.  The Active Inference Selector is
deterministic given the same candidates and exploration weight.

\textbf{Expected runtime:} Approximately 40--60 seconds per prompt on
a single GPU (attribution graph generation: $\sim$18s; 40 ablations at
$\sim$30ms each: $\sim$1.2s; overhead: $\sim$20s for model setup on
first prompt).

\textbf{Raw results:} All experiment outputs are saved as JSON in the
\texttt{results/} directory, including per-prompt KL values for all
methods, feature IDs, layer distributions, and steering outcomes.
