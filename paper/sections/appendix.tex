% ---- appendix.tex ----

\section{Active Inference Selector Parameters}
\label{app:selector}

This appendix details the initialisation and hyperparameters of the
Active Inference Selector.

\textbf{Exploration weight ($\omega_e$).}  Default value: 2.0.  Higher
values increase early-stage exploration of uncertain features at the
cost of potentially delaying exploitation of known high-value features.
The value was selected based on the IOI benchmark to balance oracle
efficiency with baseline improvement.

\textbf{Uncertainty initialisation.}  All features start with $u(i) = 1$.
After observing feature $i$, its uncertainty drops to 0.  Same-layer
features receive a 30\% reduction ($u(j) \leftarrow 0.7 \cdot u(j)$),
and adjacent-position features receive a 10\% reduction
($u(j) \leftarrow 0.9 \cdot u(j)$).

\textbf{Layer prior.}  Initialised to $\lambda_\ell = 1$ for all layers.
Updated after each observation as
$\lambda_\ell = 1 + 0.5 (\bar{\text{KL}}_\ell / \bar{\text{KL}}_{\text{global}} - 1)$.

\textbf{Candidate extraction.}  Up to 5 features per layer, 60 total
candidates, selected from pruned graph features sorted by influence
(sum of absolute adjacency weights).  Pruning thresholds: node = 0.8,
edge = 0.98 (defaults from \texttt{circuit-tracer}).


\section{Correspondence Metric}
\label{app:correspondence}

The primary metric for evaluating selector quality is oracle efficiency:
the ratio of cumulative KL divergence achieved by the selector to that
achieved by the oracle (features sorted by true KL divergence,
descending):
\begin{equation}
  \text{Oracle Efficiency} = \frac{\sum_{t=1}^{B} \text{KL}_{i_t^{\text{method}}}}
                                  {\sum_{t=1}^{B} \text{KL}_{i_t^{\text{oracle}}}}
                             \times 100\%
\end{equation}

Secondary metrics include mean KL per intervention (higher = better
feature selection) and improvement percentages over random and greedy
baselines.

KL divergence between clean and intervened output distributions is
computed as:\footnote{The implementation calls
\texttt{kl\_div(log(p\_interv), p\_clean)}, which in PyTorch's
convention yields $D_{\text{KL}}(p_{\text{clean}} \| p_{\text{interv}})$.
This direction measures the information lost when the intervened
distribution is used to approximate the clean one, and is the natural
choice for quantifying the causal impact of an ablation.}
\begin{equation}
  D_{\text{KL}}(p_{\text{clean}} \| p_{\text{interv}}) =
  \sum_v p_{\text{clean}}(v) \log \frac{p_{\text{clean}}(v)}{p_{\text{interv}}(v)}
\end{equation}
using \texttt{torch.nn.functional.kl\_div} with a $10^{-10}$ smoothing
factor to prevent numerical instability.


\section{Docker and DGX Spark Deployment}
\label{app:docker}

The \ACD{} framework is packaged as a Docker container targeting the
NVIDIA DGX Spark platform (GB10 GPU, 128~GB unified memory, CUDA~12.8,
aarch64 architecture).

\begin{lstlisting}[language=bash,caption={Docker build and run for DGX Spark.},breaklines=true]
# Build
docker compose build active-circuit-discovery
# Run experiments
docker compose run active-circuit-discovery \
  python -m src.experiments.run_real_experiments
# Results saved to results/
\end{lstlisting}

The \texttt{docker-compose.yml} in the repository root provides volume
mounts for model weights (cached from HuggingFace Hub) and result
directories.  The container installs \texttt{circuit-tracer} from source
and pins all dependencies via \texttt{requirements.txt}.


\section{Code-Level Fixes Applied}
\label{app:fixes}

Table~\ref{tab:fixes} summarises the critical API fixes discovered
during the implementation and validation process.

\begin{table*}[t]
  \centering
  \caption{Summary of critical API fixes applied to the \ACD{} codebase.}
  \label{tab:fixes}
  \footnotesize
  \newcolumntype{L}{>{\raggedright\arraybackslash}X}
  \begin{tabularx}{\textwidth}{l l L L}
    \toprule
    \textbf{File} & \textbf{Issue} & \textbf{Error} & \textbf{Fix} \\
    \midrule
    \texttt{ct\_backend.py} & LogitTarget attr.\ &
      \texttt{.token} does not exist on \texttt{LogitTarget} &
      Changed to \texttt{.token\_str} \\
    \texttt{ct\_backend.py} & PruneResult &
      Assumed \texttt{prune\_result.graph} exists &
      Apply masks to raw adjacency matrix \\
    \texttt{interv\_engine.py} & Model access &
      \texttt{model.model.cfg} fails &
      Use \texttt{model.cfg} directly \\
    \texttt{interv\_engine.py} & Intervention &
      Used \texttt{run\_with\_hooks} (residual stream) &
      Use \texttt{feature\_intervention} API \\
    \texttt{runner.py} & Fake data &
      \texttt{np.random.beta/normal} for validation &
      Real measurements only \\
    \texttt{pomdp\_agent.py} & Skip bias &
      ``skip'' action 90\% of the time &
      Custom ActiveInferenceSelector \\
    \bottomrule
  \end{tabularx}
\end{table*}


\section{Experimental Reproducibility Checklist}
\label{app:repro}

\textbf{Model and data availability:} Gemma-2-2B is available from
HuggingFace Hub (\texttt{google/gemma-2-2b}) under the Gemma license.
GemmaScope transcoders are loaded automatically by \texttt{circuit-tracer}.

\textbf{Hardware requirements:} A single NVIDIA GPU with at least 8~GB
VRAM (tested on T4 with 16~GB in Colab and GB10 with 128~GB on DGX Spark).
The model loads in float32 and requires approximately 5~GB VRAM.

\textbf{Software environment:} All dependencies are pinned in
\texttt{requirements.txt}.  Key packages: \texttt{circuit-tracer}
(from git), \texttt{transformer-lens}, \texttt{torch>=2.0}.

\textbf{Random seed:} Random baselines use
\texttt{numpy.random.seed(42)}.  The Active Inference Selector is
deterministic given the same candidates and exploration weight.

\textbf{Expected runtime:} Approximately 40--60 seconds per prompt on
a single GPU (attribution graph generation: $\sim$18s; 40 ablations at
$\sim$30ms each: $\sim$1.2s; overhead: $\sim$20s for model setup on
first prompt).

\textbf{Raw results:} All experiment outputs are saved as JSON in the
\texttt{results/} directory, including per-prompt KL values for all
methods, feature IDs, layer distributions, and steering outcomes.
