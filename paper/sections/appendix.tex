% ---- appendix.tex ----

\section{Generative Model Parameter Initialisation}
\label{app:genmodel}

This appendix details the initialisation procedure for the four generative model
parameter arrays used by the \pymdp\ agent.

\textbf{Observation model $\mat{A}$.} Given $N$ active features and 3 observation
categories (low, medium, high effect), $\mat{A} \in \mathbb{R}^{3 \times (N+1)}$.
Feature $k$ with normalised activation $\hat{a}_k = a_k / \max_j a_j$ is initialised
as:
\begin{equation}
  \mat{A}_{:,k} = \operatorname{softmax}\!\bigl(\bigl[1 - \hat{a}_k,\;
                  0.5,\; \hat{a}_k\bigr] \cdot \beta\bigr)
\end{equation}
with temperature $\beta = 2.0$. The irrelevant-feature column ($k = N+1$) is
initialised as $\mat{A}_{:,N+1} = [0.8, 0.15, 0.05]^\top$ (predominantly low effect).

\textbf{Transition model $\mat{B}$.} For each action $u \in \{0, 1, 2\}$ corresponding
to \{ablation, patching, mean ablation\}, $\mat{B}^{(u)} \in \mathbb{R}^{(N+1)\times(N+1)}$
is initialised as $\mat{I}_{N+1} \cdot 0.9 + 0.1/(N+1)$ (predominantly self-transition
with slight diffusion). After observing a high-effect ablation of feature $k$, the
transition probabilities are updated to reflect that features in the same layer are
less likely to be the unique circuit member (reducing redundant interventions).

\textbf{Preference vector $\vect{c}$.} The log-preference over observations is set to
$\vect{c} = [0, 0.5, 1.0]^\top$, preferring high-effect observations as these
provide more information about circuit structure. The values are not renormalised so
that they function as linear utilities rather than log-probabilities.

\textbf{Prior over initial states $\vect{d}$.} A uniform prior $d_k = 1/(N+1)$ is
used, reflecting initial ignorance about feature importance. An informative prior
derived from gradient-based scores would be a straightforward extension.

\section{Correspondence Metric Derivation}
\label{app:correspondence}

Let $\vect{\pi}_T = (\pi_{T,1}, \ldots, \pi_{T,N})$ be the posterior importance vector
after $T$ interventions and $\vect{e} = (e_1, \ldots, e_N)$ the vector of empirical
direct effects (computed by ablating each feature once). The Spearman rank correlation
is:
\begin{equation}
  \rho_s(\vect{\pi}_T, \vect{e}) = 1 - \frac{6 \sum_{k=1}^N d_k^2}{N(N^2-1)},
\end{equation}
where $d_k = \operatorname{rank}(\pi_{T,k}) - \operatorname{rank}(e_k)$ is the
difference in ranks. Statistical significance is assessed using a two-tailed
permutation test (10,000 permutations) rather than the asymptotic $t$-approximation,
which requires $N \geq 10$.

The choice of Spearman over Pearson correlation is motivated by three considerations.
First, $\pi_{T,k}$ are posterior probabilities in a simplex ($\sum_k \pi_{T,k} = 1$)
while $e_k$ are logit differences in $\mathbb{R}$; there is no reason to expect a
linear relationship between these quantities. Second, SAE feature activations and
intervention effects are frequently heavy-tailed, making rank-based measures more
robust~\cite{MacKay2003}. Third, Spearman correlation is undefined for $N = 1$
(unlike Pearson, which silently returns undefined/NaN), providing a natural safeguard
against the single-sample failure mode identified in the pilot.

\section{Docker and DGX Spark Deployment}
\label{app:docker}

The \ACD\ framework is packaged as a multi-stage Docker container targeting the NVIDIA
DGX Spark platform. The container is based on the NVIDIA PyTorch NGC image
\texttt{nvcr.io/nvidia/pytorch:24.10-py3} which provides CUDA~12.6, cuDNN~9.5,
and PyTorch~2.5 pre-installed with NVLink and NVMe optimisations for DGX hardware.

\begin{lstlisting}[language=bash,caption={Docker build and run for DGX Spark.}]
# Build
docker build -t acd:latest \
  --build-arg PLATFORM=dgx-spark .

# Single-GPU run
docker run --gpus '"device=0"' \
  -v /data/acd:/workspace/results \
  acd:latest python experiments/golden_gate_bridge.py

# Multi-GPU with DGX Spark NVLink
docker run --gpus all \
  --ipc=host --ulimit memlock=-1 \
  -e NCCL_IB_DISABLE=0 \
  -v /data/acd:/workspace/results \
  acd:latest python run_complete_experiment.py \
  --distributed --world-size 4
\end{lstlisting}

The \texttt{docker-compose-dgx.yml} file in the repository root provides a complete
orchestration configuration including volume mounts for model weights (shared from
the DGX Spark's NFS-mounted model cache), result directories, and logging endpoints.
Environment variables \texttt{HF\_HOME} and \texttt{SAE\_LENS\_CACHE} are set to the
shared cache path to avoid redundant downloads of the 2.4~GB GPT-2 Small SAE weights.

For multi-node DGX Spark cluster execution, the experiment runner supports
\texttt{torch.distributed} via the \texttt{--distributed} flag, partitioning the
prompt set across GPUs with each GPU independently executing its assigned interventions.
Attribution graph aggregation and final correspondence computation are performed on
GPU~0 after all workers complete.

\section{Code-Level API Fixes Applied}
\label{app:fixes}

Table~\ref{tab:fixes} summarises all API-level fixes applied to the codebase as a
result of the critical review, with the file, line range, error description, and fix
applied.

\begin{table*}[t]
  \centering
  \caption{Summary of critical API fixes applied to the \ACD\ codebase.}
  \label{tab:fixes}
  \small
  \begin{tabularx}{\textwidth}{llXX}
    \toprule
    File & Lines & Error & Fix Applied \\
    \midrule
    \texttt{tracer.py} & 96--97 & Single-string SAE ID not valid for SAE-Lens v0.3+ &
      Replaced with \texttt{SAE.from\_pretrained(release, sae\_id)} tuple call \\
    \texttt{tracer.py} & 182--194 & Fallback SAE uses \texttt{torch.randn} random weights;
      all analyses on random projections &
      Retained fallback but added \texttt{require\_sae} config flag to abort rather
      than silently proceed \\
    \texttt{agent.py} & 388 & \texttt{infer\_states(array)} rather than \texttt{infer\_states([idx])} &
      Wrapped observation in list: \texttt{[obs\_idx]} \\
    \texttt{agent.py} & 399 & \texttt{update\_A} does not exist in \pymdp\ v0.0.1 &
      Replaced with \texttt{update\_likelihood\_dirichlet(qs, observations)} \\
    \texttt{agent.py} & 284--307 & Convergence compared importance of feature $i$ with
      importance of feature $j$ (different features) &
      Replaced with symmetric KL divergence between successive belief
      distributions (Eq.~\ref{eq:convergence}) \\
    \texttt{metrics.py} & 143--200 & Pearson $r$ at $n=1$; always returns 0 &
      Changed to Spearman $\rho$ accumulated over all interventions and computed
      once at end of experiment \\
    \texttt{runner.py} & 522--555 & \texttt{\_generate\_prediction\_test\_data()} used
      \texttt{np.random.beta()} and \texttt{np.random.normal()} as
      fake validation data &
      Removed; replaced with real circuit measurements extracted from
      TransformerLens hooks during the intervention loop \\
    \texttt{prediction\_system.py} & 169 & \texttt{circuit\_graph.nodes.items()} fails
      because \texttt{nodes} is a \texttt{List}, not a \texttt{Dict} &
      Changed to iterate as \texttt{for node in circuit\_graph.nodes} \\
    \texttt{prediction\_system.py} & 244 & \texttt{circuit\_graph.edges.values()} fails
      because \texttt{edges} is a \texttt{List}, not a \texttt{Dict} &
      Changed to \texttt{[edge.weight for edge in circuit\_graph.edges]} \\
    \texttt{tracer.py} & 448--449 & \texttt{\_perform\_mean\_ablation} called
      \texttt{\_perform\_ablation\_intervention} (zero, not mean) &
      Implemented true corpus-mean computation over 100-sentence reference set \\
    \bottomrule
  \end{tabularx}
\end{table*}

\section{Experimental Reproducibility Checklist}
\label{app:repro}

The following checklist consolidates the requirements for reproducing the \ACD\
experiments. It follows the NeurIPS reproducibility guidelines~\cite{Conmy2023}.

Model and data availability: GPT-2 Small is available from HuggingFace Hub
(\texttt{gpt2}) under the MIT licence. SAE weights for \texttt{gpt2-small-res-jb}
are available from SAE-Lens under the Apache 2.0 licence.

Hardware requirements: a single NVIDIA GPU with at least 16~GB VRAM (tested on
L40S with 48~GB). CPU-only execution is possible but expected to increase runtime
by approximately 100x.

Software environment: all dependencies are pinned in \texttt{requirements.txt}.
The key version constraints are \texttt{pymdp==0.0.1},
\texttt{transformer-lens==2.16.0}, \texttt{sae-lens>=0.3.0}.

Random seed: the Active Inference agent uses no stochastic operations beyond the
policy sampling step, which is seeded with \texttt{numpy.random.seed(42)}. Reproducible
results require setting the global PyTorch seed before model loading.

Expected runtime: approximately 120 seconds per prompt template on a single L40S
GPU, assuming 20 interventions at 15--30~ms each and additional overhead for SAE
encoding and graph construction.
